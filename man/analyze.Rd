% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/analyze.R
\name{analyze}
\alias{analyze}
\title{Analyze synthetic data}
\usage{
analyze(
  formula,
  implicates,
  donor.N,
  sample_weights = NULL,
  replicate_weights = NULL,
  static = NULL,
  by = NULL,
  var.scale = 4,
  cores = 1
)
}
\arguments{
\item{formula}{A formula describing the desired analysis. Example: \code{var1 ~ 1} requests mean and total if \code{var1} is numeric or proportions for individual levels if \code{var1} is a factor. Regression model formulas can be specified as in \code{\link[stats]{glm}}.}

\item{implicates}{Data frame containing implicates of synthetic (fused) variables. Typically generated by \link{fuse}. The implicates should be row-stacked and identified by integer column "M".}

\item{donor.N}{Number of (un-weighted) observations in the original "donor" data used to generate the implicates.}

\item{sample_weights}{Vector of primary sampling weights. If absent, equal weights are assumed.}

\item{replicate_weights}{Optional data frame where each column is a vector of survey replicate weights. \code{nrow(replicate_weights)} must equal \code{length(sample_weights)}. If \code{NULL}, standard errors are calculated without using replicate weights.}

\item{static}{Optional data frame containing static (non-synthetic) variables that do not vary across implicates. These are variables either required by \code{formula} or \code{by}.}

\item{by}{Character. Optional column name(s) in \code{implicates} or \code{static} (typically factors) that collectively define the population subgroups across which the analysis will be performed. If \code{NULL}, analysis is performed on the whole sample.}

\item{var.scale}{Scalar. Factor by which to scale the standard variance in the presence of replicate weights. This is determined by the survey design. The default (\code{var.scale = 4}) is appropriate for both RECS and ACS.}

\item{cores}{Integer. Number of cores used. Only applicable on Unix systems.}
}
\value{
A tibble reporting analysis results, possibly across population subgroups defined in \code{by}. The returned quantities include:

\describe{
\item{estimate}{Point estimate. Mean of estimates across implicates.}
\item{std_error}{Standard error of the point estimate.}
\item{lower_ci}{Lower bound of 95\% confidence interval.}
\item{upper_ci}{Upper bound of 95\% confidence interval.}
\item{statistic}{Test statistic; z-value for proportions and t-value otherwise.}
\item{pvalue}{p-value of the test statistic, rounded to five decimals.}
\item{degf}{Degrees of freedom. Zero for proportions. \code{N - 1} if a single implicate is provided.}
\item{nobs}{Mean number of observations across the implicates.}
}
}
\description{
Calculation of point estimates and associated standard errors for analyses using fused/synthetic microdata. Can calculate means, proportions, and linear regression coefficients, optionally performed across population subgroups.
}
\details{
At a minimum, the user must supply synthetic implicates (typically generated by \link{fuse}). Inputs are checked for consistent dimensions.

If \code{implicates} contains only a single implicate and \code{replicate_weights = NULL}, the "typical" standard error is returned with a warning to make sure the user is aware of the situation.

Estimates and standard errors for the requested analysis are calculated separately for each implicate. The final point estimate is the mean estimate across implicates. The final standard error is the pooled SE across implicates, calculated using Rubin's pooling rules (1987) with a finite population adjustment of the degrees of freedom (Barnard and Rubin 1999).

When replicate weights are provided, the standard errors of each implicate are calculated via the variance of estimates across replicates. Calculations leverage \code{\link[data.table]{data.table}} operations for speed and memory efficiency. The within-implicate variance is calculated around the point estimate (rather than around the mean of the replicates). This is equivalent to \code{mse = TRUE} in \code{\link[survey]{svrepdesign}}. This seems to be the appropriate method for most surveys.

If replicate weights are NOT provided, the standard errors of each implicate are calculated using variance within the implicate. For means, the ratio variance approximation of Cochran (1977) is used, as this is known to be a good approximation of bootstrapped SE's for weighted means (Gatz and Smith 1995). For proportions, a generalization of the unweighted SE formula is used (\href{https://stats.stackexchange.com/questions/159204/how-to-calculate-the-standard-error-of-a-proportion-using-weighted-data}{see here}). For regression coefficients, the standard error is calculated by \code{\link[stats]{summary.glm}}.
}
\examples{
# Build a fusion model using RECS microdata
fusion.vars <- c("electricity", "natural_gas", "aircon")
predictor.vars <- names(recs)[2:12]
train(data = recs, y = fusion.vars, x = predictor.vars, file = "test_model.fsn")

# Generate 10 implicates of the 'fusion.vars' using original RECS as the recipient
recipient <- recs
sim <- fuse(data = recs, file = "test_model.fsn", M = 10)
head(sim)

#---------

# Analyze simulated electricity consumption,
# by climate zone, without replicate weights
result1 <- analyze(formula = electricity ~ 1,
                  implicates = sim,
                  sample_weights = recipient$weight,
                  replicate_weights = NULL,
                  static = recipient,
                  by = "climate",
                  donor.N = nrow(recs))

# Analyze simulated electricity consumption,
# by climate zone, WITH replicate weights
result2 <- analyze(formula = electricity ~ 1,
                   implicates = sim,
                   sample_weights = recipient$weight,
                   replicate_weights = recipient[startsWith(names(recipient), "rep_")],
                   static = recipient,
                   by = "climate",
                   donor.N = nrow(recipient))

# Helper function for comparison plots
pfun <- function(x, y) {plot(x, y); abline(0, 1, lty = 2)}

# Inclusion of replicate weights does not affect estimates, but it does
# increase standard errors considerably due to RECS small sample size
pfun(result1$estimate, result2$estimate)
pfun(result1$std_error, result2$std_error)

#---------

# Conduct the same analyses using the original RECS data
# This generates a warning, since only an (implicit) single implicate is provided

# Without replicate weights
orig1 <- analyze(formula = electricity ~ 1,
                implicates = recs,
                sample_weights = recs$weight,
                replicate_weights = NULL,
                static = recs,
                by = "climate",
                donor.N = nrow(recs))

# And WITH replicate weights
orig2 <- analyze(formula = electricity ~ 1,
                 implicates = recs,
                 sample_weights = recs$weight,
                 replicate_weights = recs[startsWith(names(recs), "rep_")],
                 static = recs,
                 by = "climate",
                 donor.N = nrow(recs))

#---------

# Other examples of analyze() syntax and usage

# Proportions of air conditioning technologies, by race, with replicate weights
result <- analyze(formula = aircon ~ 1,
                  implicates = sim,
                  sample_weights = recipient$weight,
                  replicate_weights = select(recipient, starts_with("rep_")),
                  static = recipient,
                  by = "race",
                  donor.N = nrow(recipient))

# Linear regression, full sample, without replicate weights
result <- analyze(formula = electricity ~ income + aircon + climate,
                  implicates = sim,
                  sample_weights = recipient$weight,
                  replicate_weights = NULL,
                  static = recipient,
                  by = NULL,
                  donor.N = nrow(recipient))

# Natural gas means and totals, by heating technology and climate, without replicate weights
result <- analyze(formula = natural_gas ~ 1,
                  implicates = sim,
                  sample_weights = recipient$weight,
                  replicate_weights = NULL,
                  static = recipient,
                  by = c("heat_type", "climate"),
                  donor.N = nrow(recipient))

#---------

# Testing replicate weight calculations
# Confirm that analyze() can reproduce results from 'survey' package

# Create 'survey' package RECS survey design object
recs.design <- survey::svrepdesign(
  data = recs,
  weights = recs$weight,
  repweights = select(recs, starts_with("rep_")),
  type = "Fay",
  rho = 0.5,
  mse = TRUE
)

 # Perform checks for mean and total electricity, by climate
check <- survey::svyby(electricity ~ 1, by = recs$climate, recs.design, FUN = survey::svymean)
test <- analyze(formula = electricity ~ 1,
                implicates = recs,
                sample_weights = recs$weight,
                replicate_weights = select(recs, starts_with("rep_")),
                by = "climate",
                donor.N = nrow(recs))

# Confirm that results are identical
all.equal(check$electricity, filter(test, metric == "mean")$estimate)
all.equal(check$se, filter(test, metric == "mean")$std_error)

}
\references{
Barnard, J., & Rubin, D.B. (1999). Small-sample degrees of freedom with multiple imputation. \emph{Biometrika, 86}, 948-955.

Cochran, W. G. (1977). \emph{Sampling Techniques} (3rd Edition). Wiley, New York.

Gatz, D.F., and Smith, L. (1995). The Standard Error of a Weighted Mean Concentration — I. Bootstrapping vs Other Methods. \emph{Atmospheric Environment}, vol. 29, no. 11, 1185–1193.

Rubin, D.B. (1987). \emph{Multiple imputation for nonresponse in surveys}. Hoboken, NJ: Wiley.
}
