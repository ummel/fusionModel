% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/validate.R
\name{validate}
\alias{validate}
\title{Validate fusion output}
\usage{
validate(
  observed,
  simulated,
  sample_weights = NULL,
  replicate_weights = NULL,
  nsets = 100,
  var_scale = 4,
  min_size = 20,
  cores = 1
)
}
\arguments{
\item{observed}{Data frame. Observed data against which to validate the \code{simulated} variables. Typically the same dataset used to \code{\link{train}} the fusion model that one seeks to validate. Need not be limited to fusion variables (see Details).}

\item{simulated}{Data frame. Simulated fusion variables. Typically the output from \code{\link{fuse}} containing multiple implicates. The implicates should be row-stacked and identified by integer column "M".}

\item{sample_weights}{Numeric. Vector of primary sampling weights with length equal to \code{nrow(observed)}.}

\item{replicate_weights}{Data frame. Each column is a vector of survey replicate weights. \code{nrow(replicate_weights)} must equal \code{length(sample_weights)}.}

\item{nsets}{Integer. Number of population subsets to analyze. A higher number generally produces more reliable validation results but takes longer to compute. See Details.}

\item{var_scale}{Scalar. Factor by which to scale the standard variance in the presence of replicate weights. This is determined by the survey design associated with \code{observed}. The default (\code{var.scale = 4}) is appropriate for both RECS and ACS.}

\item{min_size}{Integer. Minimum number of observations allowed in a subset.}

\item{cores}{Integer. Number of cores used. Only applicable on Unix systems.}
}
\value{
A list with slots named "estimates" and "correlation". This data does not usually need to be interrogated, since users are more likely to pass the result to \code{\link{plot_valid}} for visualization.

\emph{estimates}: Data frame giving observed (m0, se0) and synthetic (m1, se1) point estimates and standard errors for each fusion variable (y) and population subset (iset). Column 'pvalue' gives the p-value associated with a statistical test where the null hypothesis is that 'm0' and 'm1' are identical.

\emph{correlation}: Data frame summarizing the difference (error) between synthetic and observed data for pairwise correlation coefficients of each fusion variable (y) and all other variables (correlate). Column 'error' gives the mean absolute error; 'error_w' gives the mean absolute error weighted by the square of the observed correlation (i.e. weighted by variance explained). In the case of categorical 'y' or 'correlate', the underlying correlations are calculated using binary vectors and the returned results reflect the mean absolute error across all levels.
}
\description{
Performs specific (non-general) internal validation exercises on fused microdata to estimate how well the simulated data reflect statistical patterns in an analogous observed dataset. This provides a standard approach to validating a fusion model created by \code{\link{train}}. See Examples for recommended usage.
}
\details{
The objective of \code{validate()} is to reveal the utility of the synthetic data across myriad analyses. Utility here is based on comparison of point estimates and standard errors derived using multiple-implicate synthetic data with those derived using the original donor data and replicate weights. The specific analyses tested include variable levels (means and proportions) and bivariate relationships (Pearson correlation coefficients) for both the full sample and relevant population subsets of varying size. This allows us to estimate how each of the synthetic variables perform in analyses with real-world relevance, at varying levels of complexity.

In effect, \code{validate()} performs a large number of analyses of the kind that the \code{\link{analyze}} function is designed to do on a one-by-one basis. Being purpose-built for particular kinds of analysis, \code{validate()} does this much more efficiently than repeated calls to \code{\link{analyze}}. It carries out hundreds or thousands of potential analyses separately for both the synthetic and observed data.

We assume that users are most likely to analyze a variable across subsets of the population that produce meaningful differences in outcomes. No (sane) analyst is interested in the mean of Y for a random subset, since the expected value within and without the subset is identical. But it does make sense to subset the population into (for example) homeowners and renters, if there is reason to believe that Y varies between the two groups.

Correlations among the provided variables are used to prioritize subsetting variables with larger absolute correlations with the fusion variables (i.e. subsets of likely real-world relevance). In addition, subsetting variables are selected so as to over-represent smaller subsets. The universe of subsets containing 95\% of observations exhibit comparatively little variation compared to those containing just 5\%; the latter subsets result in noisier validation results. Consequently, the returned analyses are biased toward smaller subsets in order to derive more reliable estimates of small-sample utility.

A total of \code{nsets} subsets are constructed. The mean/proportions of each fusion variable are returned for each of the \code{nsets} subsets, as is the pairwise (bivariate) Pearson correlation coefficient error between each fusion variable and all other variables provided in \code{observed}. See Value for details.

If there is not sufficient data provided to construct \code{nsets} subsets, then random subsets are used to make up the difference. For purposes of subset creation, each continuous variable is converted to five cumulative binary variables on the basis of a univariate \code{\link[stats]{kmeans}} clustering.
}
\examples{
# Build a fusion model using RECS microdata
# Note that "test_model.fsn" will be written to working directory
fusion.vars <- c("electricity", "natural_gas", "aircon")
predictor.vars <- names(recs)[2:12]
train(data = recs,
      y = fusion.vars,
      x = predictor.vars,
      file = "test_model.fsn",
      weight = "weight")

# Fuse back onto the donor data (multiple implicates)
# Since the result is intended for validation, we set 'ignore_self = TRUE'
sim <- fuse(data = recs,
            file = "test_model.fsn",
            M = 20,
            ignore_self = TRUE)

# Calculate validation results
valid <- validate(observed = recs[, c(fusion.vars, predictor.vars)],
                  simulated = sim,
                  sample_weights = recs$weight,
                  replicate_weights = subset(recs, select = rep_1:rep_96))

}
