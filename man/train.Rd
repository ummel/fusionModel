% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/train.R
\name{train}
\alias{train}
\title{Train a fusion model}
\usage{
train(
  data,
  y,
  x,
  file = "fusion_model.fsn",
  weight = NULL,
  nfolds = 5,
  ptiles = c(0.25, 0.75),
  hyper = NULL,
  threads = 1
)
}
\arguments{
\item{data}{Data frame. Donor dataset. Categorical variables must be factors and ordered whenever possible.}

\item{y}{Character or list. Variables in \code{data} to eventually fuse to a recipient dataset. Variables are fused in the order provided. If \code{y} is a list, each entry is a character vector possibly indicating multiple variables to fuse as a block.}

\item{x}{Character. Predictor variables in \code{data} common to donor and eventual recipient.}

\item{file}{Character. File where fusion model will be saved. Must use \code{.fsn} suffix.}

\item{weight}{Character. Name of the observation weights column in \code{data}. If NULL (default), uniform weights are assumed.}

\item{nfolds}{Numeric. Number of cross-validation folds used for LightGBM model training. Or, if \code{nfolds < 1}, the fraction of observations to use for training set; remainder used for validation (faster than cross-validation).}

\item{ptiles}{Numeric. One or more percentiles for which quantile models are trained for continuous \code{y} variables (along with the conditional mean).}

\item{hyper}{List. LightGBM hyperparameters to be used during model training. If \code{NULL}, default values are used. See Details and Examples.}

\item{threads}{Integer. Number of threads used for LightGBM parallel operations. \code{threads = 0} will use all threads detected by OpenMP. NOTE: This may change in future.}
}
\value{
A fusion model object (.fsn) is saved to \code{file}.
}
\description{
Train a fusion model on "donor" data using sequential \href{https://lightgbm.readthedocs.io/en/latest/}{LightGBM} models to model the characteristics of conditional distributions. The resulting fusion model (.fsn file) can used with \link{fuse} to simulate outcomes for a "recipient" dataset.
}
\details{
When \code{y} is a list, each slot indicates either a single variable or, alternatively, multiple variables to fuse as a block. Variables within a block are sampled jointly from the original donor data during fusion. See Examples.

The fusion model written to \code{file} is a zipped archive created by \code{\link[zip]{zip}} containing models and data required by \link{fuse}.

The \code{hyper} argument can be used to specify the LightGBM hyperparameter values over which to perform a "grid search" during model training. \href{https://lightgbm.readthedocs.io/en/latest/Parameters.html}{See here} for the full list of parameters. For each combination of hyperparameters, \code{nfolds} cross-validation is performed using \code{\link[lightgbm]{lgb.cv}} with an early stopping condition. The parameter combination with the lowest loss function value is used to fit the final model via \code{\link[lightgbm]{lgb.train}}. The more candidate parameter values specified in \code{hyper}, the longer the processing time. If \code{hyper = NULL}, a single set of parameters is used LightGBM default values. Typically, users will only have reason to specify the following parameters via \code{hyper}:

\itemize{
\item boosting
\item num_leaves
\item bagging_fraction
\item feature_fraction
\item min_data_in_leaf
\item num_iterations
\item learning_rate
\item max_bin
\item min_data_in_bin
}
}
\examples{
# Build a fusion model using RECS microdata
# Note that "test_model.fsn" will be written to working directory
?recs
fusion.vars <- c("electricity", "natural_gas", "aircon")
predictor.vars <- names(recs)[2:12]
train(data = recs, y = fusion.vars, x = predictor.vars, file = "test_model.fsn")

# When 'y' is a list, it can specify variables to fuse as a block
fusion.vars <- list("electricity", "natural_gas", c("heating_share", "cooling_share", "other_share"))
fusion.vars
train(data = recs, y = fusion.vars, x = predictor.vars, file = "test_model.fsn")

# Specify a single set of LightGBM hyperparameters
train(data = recs, y = fusion.vars, x = predictor.vars, file = "test_model.fsn",
      hyper = list(boosting = "goss",
                   feature_fraction = 0.8,
                   num_iterations = 300
      ))

# Specify a range of LightGBM hyperparameters to search over
# This takes longer, because there are more models to test
train(data = recs, y = fusion.vars, x = predictor.vars, file = "test_model.fsn",
      hyper = list(num_leaves = c(10, 30),
                   feature_fraction = c(0.8, 0.9, 1),
                   num_iterations = 50
      ))
}
