% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/trainCART.R
\name{trainCART}
\alias{trainCART}
\title{Train a fusion model using Classification and Regression Trees (CART)}
\usage{
trainCART(
  data,
  y,
  x,
  weight = NULL,
  order = NULL,
  deriv = TRUE,
  smoothed = FALSE,
  cores = 1,
  lasso = NULL,
  maxcats = 12,
  complexity = 0,
  cvfolds = 0,
  cvfactor = 0,
  node.obs = c(20, 10),
  initial = c(1, 0)
)
}
\arguments{
\item{data}{Data frame. Donor dataset. Categorical variables must be factors and ordered whenever possible.}

\item{y}{Character vector. The variables to fuse to a recipient dataset.}

\item{x}{Character vector. Predictor variables common to donor and eventual recipient.}

\item{weight}{Character vector. Name of the observation weights column. If NULL (default), uniform weights are assumed.}

\item{order}{Character vector. Order in which to fuse \code{y}. If NULL (default), a pseudo-optimal order is determined internally.}

\item{deriv}{Logical. Should algorithm check for derivative relationships prior to building fusion models?}

\item{smoothed}{Logical. Should the synthetic values be smoothed via KDE? Default is \code{FALSE}.}

\item{cores}{Integer. Number of cores used for parallel operations. Passed to \code{cl} argument of \code{\link[pbapply]{pblapply}}. Ignored on Windows systems.}

\item{lasso}{Numeric (0-1) or NULL. Controls extent of predictor variable pre-screening via LASSO regression. If NULL (default), no screening is performed. \code{lasso = 1} invokes the least-restrictive screening. See Details.}

\item{maxcats}{Positive integer or NULL. Maximum number of levels allowed in an unordered factor predictor variable when the response (fusion) variable is also an unordered factor. Prevents excessive \code{\link[rpart]{rpart}} computation time. A K-means clustering strategy is used to cluster the predictor to no more than \code{maxcats} levels.}

\item{complexity}{Numeric. Passed to \code{cp} argument of \code{\link[rpart]{rpart.control}} to control complexity of decision trees.}

\item{cvfolds}{Integer. Number of cross-validation folds used by \code{\link[rpart]{rpart}} to determine optimal tree complexity. Default is no cross-validation (\code{cvfolds = 0}).}

\item{cvfactor}{Numeric. Controls how decision trees are pruned when \code{cvfolds > 0}. \code{cvfactor = 0} (the default) selects the tree complexity that minimizes the cross-validation error. \code{cvfactor = 1} is equivalent to Breiman's "1-SE" rule.}

\item{node.obs}{Numeric vector of length 2. Minimum number of observations in tree nodes. First number is for numeric response variables; second for categorical. Each is passed to \code{minbucket} argument of \code{\link[rpart]{rpart.control}}.}

\item{initial}{Numeric vector of length 2. Controls speed/performance of the initial model-fitting routune used to determine fusion order. First number is proportion of \code{data} observations to randomly sample. Second number is the \code{cp} argument passed to \code{\link[rpart]{rpart.control}}. See Details.}
}
\value{
A list containing trained model information to be passed to \link{fuse}.
}
\description{
Train a fusion model on donor data using CART implemented via \code{\link[rpart]{rpart}}.
}
\details{
When \code{lasso} is non-NULL, predictor variables are "pre-screened" using LASSO regression via \code{\link[glmnet]{glmnet}} prior to fitting a \code{\link[rpart]{rpart}} model. Predictors with a LASSO coefficient of zero are excluded from consideration. This can speed up tree-fitting considerably when \code{data} is large. Lower values of \code{lasso} are more aggressive at excluding predictors; the LASSO \emph{lambda} is chosen such that the deviance explained is at least \code{lasso}-\% of the maximum. To ensure the LASSO step is fast, pre-screening is only used for numeric, logical, and ordered factor response variables (the latter integerized).

Since determination of the fusion order only makes use of variable importance results from the initial (fully-specified) models, employing a random subset of observations and less complex models (controlled via \code{initial}) can yield a competitive fusion order at less expense.
}
\examples{
# Build a fusion model using RECS microdata
?recs
fusion.vars <- c("electricity", "natural_gas", "aircon")
predictor.vars <- names(recs)[2:12]
fit <- train(data = recs, y = fusion.vars, x = predictor.vars)
}
