% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/plot_valid.R
\name{plot_valid}
\alias{plot_valid}
\title{Plot validation results}
\usage{
plot_valid(valid, y = NULL, path = NULL, cores = 1, ...)
}
\arguments{
\item{valid}{List returned by \code{\link{validate}}.}

\item{y}{Character. Fusion variables to use for validation graphics. Useful for plotting partial validation results. Default is to use all fusion variables present in \code{valid}.}

\item{path}{Character. Path to directory where .png graphics are to be saved. Directory is created if necessary. If NULL (default), no files are saved to disk.}

\item{cores}{Integer. Number of cores used. Only applicable on Unix systems.}

\item{...}{Arguments passed to \code{\link[ggplot2]{ggsave}} to control .png graphics saved to disk.}
}
\value{
A list with the following named slots, each containing a \code{\link[ggplot2]{ggplot}} object visualizing validation results for all of the fusion variables.

\itemize{
\item plot1: Validity of point estimates (absolute percent error).
\item plot2: Validity of pairwise correlation coefficients (absolute error).
\item plot3: Validity of \emph{t}-test of equality of observed and simulated point estimates (probability of p-value < 0.05).
\item plot4: Validity of standard errors (ratio of simulated SE to observed SE).
\item Additional named slots (one for each of the fusion variables) contain the same four plots described above with scatterplot of subset-specific results and a shaded 95\% confidence interval of the smoothed median.
\item est_smooth: data frame with raw smoothing results for estimates (expert use only)
\item cor_smooth: data frame with raw smoothing results for correlations (expert use only)
}
}
\description{
Creates and optionally saves to disk representative plots of validation results returned by \code{\link{validate}}. Requires the \code{\link[qgam]{qgam}}, \code{\link[ggplot2]{ggplot2}}, and \code{\link[scales]{scales}} packages.
}
\details{
The validation results are visualized with the goal of conveying expected, typical performance across the fusion variables. That is, how well do the simulated data match the observed data with respect to point estimates, pairwise correlations, and uncertainty/standard errors for both the full sample and population subsets of various size? See \code{\link{validate}} for details and rationale of subset creation.

The input validation results are converted to plausible error metrics for plotting. For comparison of point estimates, the error metric is absolute percent error for continuous variables and classification error for categorical (i.e. the proportion of the population that is misclassified in the simulated data compared to the observed). Since these metrics are similar in interpretation, results for continuous and categorical variables are plotted together.

The error metric is absolute error when comparing correlation coefficients. Uncertainty estimates are compared using a t-test of equal point estimates and the ratio of simulated-to-observed standard errors. The latter allows the user to detect systematic bias in standard errors.

For a given fusion variable, the error metric will exhibit variation even for subsets of comparable size, due to the fact that each subset looks at a unique partition of the data. In order to convey how expected performance varies with subset size, an additive quantile regression model (see \code{\link[qgam]{qgam}}) is used to calculate a smoothed, robust relationship between subset size and the validation metric. The resulting curve gives the expected, typical (median) performance, conditional on subset size.

A console message indicating "outer Newton did not converge fully" van be produced (via \code{\link[qgam]{qgam}}; it cannot be suppressed). This can generally be ignored, though it is always good practice to check the variable-specific plots to ensure the smoothed fits are plausible.
}
\examples{
# Build a fusion model using RECS microdata
# Note that "test_model.fsn" will be written to working directory
fusion.vars <- c("electricity", "natural_gas", "aircon")
predictor.vars <- names(recs)[2:12]
train(data = recs,
      y = fusion.vars,
      x = predictor.vars,
      file = "test_model.fsn",
      weight = "weight")

# Generate multiple implicates
sim <- fuse(data = recs,
            file = "test_model.fsn",
            M = 30,
            ignore_self = TRUE)

# Calculate validation results
valid <- validate(observed = recs[, c(fusion.vars, predictor.vars)],
                  simulated = sim,
                  sample_weights = recs$weight,
                  replicate_weights = subset(recs, select = rep_1:rep_96))

# Create validation plots
vplots <- plot_valid(valid)
names(vplots)

# View some of the plots
vplots$plot1
vplots$electricity$plot1
vplots$aircon$plot2

# Can also save the plots to disk at creation
# Will save .png files to 'valid_plots' folder in working directory
vplots <- plot_valid(valid,
                     path = file.path(getwd(), "valid_plots"),
                     width = 8, height = 6)

}
