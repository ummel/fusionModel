#' Fuse variables to a recipient dataset
#'
#' @description
#' Fuse variables to a recipient dataset using a .fsn model produced by \link{train} using conditional distribution matching. \code{fuseM()} provides a convenience wrapper for generating multiple implicates. Output of \code{fuseM()} can be passed to \code{analyze()}.
#'
#' @param data Data frame. Recipient dataset. All categorical variables should be factors and ordered whenever possible. Data types and levels are strictly validated against predictor variables defined in \code{model}.
#' @param file Character. Path to fusion model file (.fsn) generated by successful call to \link{train}.
#' @param k Integer. Number of nearest neighbors to identify among the donor observations.
#' @param max_dist Numeric. Controls the maximum allowable distance when identifying up to \code{k} nearest neighbors. \code{max_dist = 0} (default) means no distance restriction is applied.
#' @param idw Logical. Should inverse distance weighting be used when randomly selecting a donor observation from the \code{k} nearest neighbors?
#' @param cores Integer. Number of cores used. Only applicable on Unix systems.
#' @param ignore_nearest Logical. If \code{TRUE}, nearest neighbor in KNN search is dropped; useful for validation exercises.
#' @param ... Arguments passed to \code{fuse()}.
#' @param M Integer. Number of implicates to simulate.

#' @param seed Set random seed if needed.
#'
#' @details For each record in \code{data}, the predicted conditional distribution values are used to identify the \code{k} most-similar observations in the original donor data along the same dimensions. One of the \code{k} nearest neighbors is randomly selected to donate its observed/"real" response value for the fusion variable(s) in question. The random selection uses inverse distance weighting if \code{idw = TRUE}.
#' @details The \code{max_dist} value is a relative scaling factor. The search radius passed to \code{\link[RANN]{nn2}} is \code{max_dist * medDist}, where \code{medDist} is the median pairwise distance calculated among all of the donor observations. This approach allows \code{max_dist} to adjust the search radius in a consistent way across all fusion variables/blocks.
#'
#' @return For \code{fuse()}, a data frame with same number of rows as \code{data} and one column for each synthetic fusion variable. The order of the columns reflects the order in which they where fused.
#' @return For \code{fuseM()}, a data frame with number of rows equal to \code{M * nrow(data)}. Integer column ".M" indicates implicate assignment of each observation. Note that the ordering of recipient observations is consistent within implicates, so do not change the row order if using with \code{analyze()}.
#'
#' @examples
#' # Build a fusion model using RECS microdata
#' # Note that "test_model.fsn" will be written to working directory
#' ?recs
#' fusion.vars <- c("electricity", "natural_gas", "aircon")
#' predictor.vars <- names(recs)[2:12]
#' train(data = recs, y = fusion.vars, x = predictor.vars, file = "test_model.fsn")
#'
#' # Generate single implicate of synthetic 'fusion.vars',
#' #  using original RECS data as the recipient
#' recipient <- recs[predictor.vars]
#' sim <- fuse(data = recipient, file = "test_model.fsn")
#' head(sim)
#'
#' # Calling fuse() again produces different results
#' sim <- fuse(data = recipient, file = "test_model.fsn")
#' head(sim)
#'
#' # Generate multiple implicates via fuseM()
#' # This can be passed to 'implicates' argument in ?analyze()
#' sim <- fuseM(data = recipient, file = "test_model.fsn", M = 5)
#' head(sim)
#' table(sim$M)
#' @export

#---------------------

# Manual testing
#
# library(fusionModel)
# source("R/utils.R")
# #Example inputs
# data <- subset(recs, select = c(weight, division, urban_rural, climate, income, age, race, hh_size, televisions))
# file <- "fusion_model_test.fsn"
# idw <- TRUE
# k <- 10

#---------------------

fuse <- function(data,
                 file,
                 k = 5,
                 max_dist = 0,
                 idw = FALSE,
                 cores = 1,
                 ignore_nearest = FALSE,
                 seed = NULL) {

  stopifnot(exprs = {
    is.data.frame(data)
    file.exists(file) & endsWith(file, ".fsn")
    k > 0 & k %% 1 == 0
    max_dist >= 0
    is.logical(idw)
    cores > 0 & cores %% 1 == 0
    is.logical(ignore_nearest)
  })

  if (is.data.table(data)) data <- as.data.frame(data)
  if (!is.null(seed)) set.seed(seed)

  # Temporary directory to unzip to
  td <- tempfile()
  dir.create(td)

  # Names of files within the .fsn object
  fsn.files <- zip::zip_list(file)
  pfixes <- sort(unique(dirname(fsn.files$filename)))
  pfixes <- pfixes[pfixes != "."]

  # Load model metadata
  zip::unzip(zipfile = file, files = "metadata.rds", exdir = td)
  meta <- readRDS(file.path(td, "metadata.rds"))

  # Names, order, and 'blocks' of response variables
  yord <- meta$yorder

  # Check that necessary predictor variables are present
  xvars <- names(meta$xclass)
  miss <- setdiff(xvars, names(data))
  if (length(miss) > 0) stop("The following predictor variables are missing from 'data':\n", paste(miss, collapse = ", "))

  # Restrict 'data' to 'xvars'
  data <- data[xvars]

  # Check for appropriate class/type of predictor variables
  xclass <- meta$xclass
  xtest <- lapply(data, class)
  miss <- !map2_lgl(xclass, xtest, sameClass)
  if (any(miss)) stop("Incompatible data type for the following predictor variables:\n", paste(names(miss)[miss], collapse = ", "))

  # Check for appropriate levels of factor predictor variables
  xlevels <- meta$xlevels
  xtest <- lapply(subset(data, select = names(xlevels)), levels)
  miss <- !map2_lgl(xlevels, xtest, identical)
  if (any(miss)) stop("Incompatible levels for the following predictor variables\n", paste(names(miss)[miss], collapse = ", "))

  # Detect and impute any missing values in 'data'
  na.cols <- names(which(sapply(data, anyNA)))
  if (length(na.cols) > 0) {
    cat("Missing values imputed for the following variable(s):\n", paste(na.cols, collapse = ", "), "\n")
    for (j in na.cols) {
      x <- data[[j]]
      ind <- is.na(x)
      data[ind, j] <-  imputationValue(x, ind)
    }
  }

  # Coerce 'data' to sparse numeric matrix for use with LightGBM
  dmat <- tomat(data)
  rm(data)

  #-----

  # Run this only if not running in parallel, otherwise the messsage messes up the progress bar
  #if (!parallel) cat("Fusing donor variables to recipient...\n")

  for (i in 1:length(pfixes)) {

    cat("Processing block", i, "of", length(pfixes), "...\n")

    v <- meta$yorder[[i]]
    block <- length(v) > 1

    # LightGBM predictor variables
    xv <- meta$lgbpred[[i]]

    # Unzip the lightGBM model(s) to temp directory
    mods <- grep(pattern = glob2rx(paste0(pfixes[i], "*.txt")), x = fsn.files$filename, value = TRUE)
    zip::unzip(zipfile = file, files = mods, exdir = td)

    # Row indices where to generate predictions
    # Modified, if necessary, in next code chunk when single continuous variables has a zero model
    zind <- rep(FALSE, nrow(dmat))

    #---

    # Simulate zero values for case of single continuous variable
    if (!block & meta$ytype[v[[1]]] == "continuous") {
      m <- grep("z\\.txt$", mods, value = TRUE)
      if (length(m)) {
        mod <- lightgbm::lgb.load(filename = file.path(td, m))
        p <- predict(object = mod,
                     data = dmat[, xv],
                     reshape = TRUE)
        zind <- p > runif(n = nrow(dmat)) # Row indices where a zero is randomly simulated (TRUE for a zero)
      }
      mods <- setdiff(mods, m)
    }

    #---

    # Load and make predictions for each model
    cat("-- Predicting LightGBM models\n")
    pred <- data.table()
    for (m in mods) {
      y <- sub("_[^_]+$", "", basename(m))
      n <- sub(".txt", "", sub(paste0(y, "_"), "", basename(m), fixed = TRUE), fixed = TRUE)
      q <- substring(n, 1, 1) == "q"
      z <- substring(n, 1, 1) == "z"
      mod <- lightgbm::lgb.load(filename = file.path(td, m))
      p <- predict(object = mod, data = dmat[!zind, xv], reshape = TRUE, params = list(num_threads = cores))
      if (!is.matrix(p)) p <- matrix(p)
      p <- as.data.table(p)
      set(pred, i = NULL, j = paste0(y, "_", n, if (q | z) NULL else 1:ncol(p)), value = p)
      rm(p)
    }

    #---

    # Apply normalization, as necessary, to recipient conditional values
    norm.vars <- intersect(names(pred), names(meta$ycenter[[i]]))
    for (j in norm.vars) {
      ncenter <- meta$ycenter[[i]][j]
      nscale <- meta$yscale[[i]][j]
      if (nscale == 0) {
        set(pred, i = NULL, j = j, value = NULL)
      } else {
        val <- normalize(pred[[j]], center = ncenter, scale = nscale)
        set(pred, i = NULL, j = j, value = val)
      }
    }

    #---

    # Simulate values for case of single categorical variable
    if (!block & meta$ytype[v[[1]]] != "continuous") {
      ptile <- runif(n = nrow(pred))
      S <- if (ncol(pred) > 1) {
        for (j in 2:ncol(pred)) set(pred, i = NULL, j = j, value = pred[[j - 1]] + pred[[j]])
        for (j in 1:ncol(pred)) set(pred, i = NULL, j = j, value = ptile > pred[[j]])
        rowSums(pred)  # First level receives value of zero
      } else {
        pred[[1]] > ptile  # The binary case
      }
    }

    #---

    if (block | meta$ytype[[v[1]]] == "continuous") {

      # Get the donor conditional prediction and response variables
      x <- paste0(pfixes[i], "/donor.fst")
      zip::unzip(zipfile = file, files = x, exdir = td)
      dpred <- fst::read_fst(file.path(td, x), as.data.table = TRUE, columns = colnames(pred))
      dresp <- fst::read_fst(file.path(td, x), as.data.table = TRUE, columns = v)
      setcolorder(pred, names(dpred))

      # Apply the column weights to 'pred'
      for (j in names(pred)) set(pred, i = NULL, j = j, value = pred[[j]] * meta$colweight[[i]][[j]])

      # Ensure none of the normalized conditional values fall outside the range observed in 'dpred'
      for (j in names(pred)) set(pred, i = NULL, j = j, value = pmin(pred[[j]], max(dpred[[j]])))
      for (j in names(pred)) set(pred, i = NULL, j = j, value = pmax(pred[[j]], min(dpred[[j]])))

      #----

      # Perform K-nearest-neighbor search
      # By default, this uses parallel processing as determined by 'cores' argument
      # The 'query' argument to nn2() is split into 'cores' chunks and processed in parallel
      # This could generate memory issues if 'pred' is too large
      # Safer approach would be to detect available memory and set number of chunks dynamically
      # NOTE: The 'eps' error bound in nn2() is set dynamically (speed/precision tradeoff)
      #  This appears to be a reasonable setting based on limited testing, but not definitive

      cat("-- Finding nearest neighbors\n")

      getNN <- function(x) {
        nn <- RANN::nn2(data = dpred,
                        query = x,
                        k = k + ignore_nearest,
                        searchtype = ifelse(max_dist == 0, "standard", "radius"),
                        radius = meta$meddist[i] * max_dist,
                        eps = sqrt(meta$meddist[i]))
        # Apply potential 'fix up' for cases where there is no nearest neighbor within the search radius
        if (max_dist > 0) {
          fix <- which(nn$nn.idx[, 1] == 0)
          if (length(fix) > 0) {
            nn.fix <- RANN::nn2(data = dpred,
                                query = x[fix, ],
                                k = 1 + ignore_nearest,  # Returns single nearest-neighbor
                                searchtype = "standard",
                                eps = sqrt(meta$meddist[i]))
            nn$nn.idx[fix, 1] <- nn.fix$nn.idx
            nn$nn.dists[fix, 1] <- nn.fix$nn.dists
            rm(fix, nn.fix)
          }
        }
        return(nn)
      }

      # Attempt PCA to reduce dimensionality prior to nn2?
      # https://github.com/ummel/fusionModel/issues/29
      # pca.fit <- prcomp(x = dpred, retx = TRUE, center = FALSE, scale. = FALSE, rank. = 8)
      # dpred2 <- pca.fit$x
      # pred2 <- predict(pca.fit, newdata = pred)

      # Split 'pred' into chunks and process in parallel
      pred <- split(pred, f = rep(1:cores, each = ceiling(nrow(pred) / cores))[1:nrow(pred)])
      nn <- parallel::mclapply(pred, getNN, mc.cores = cores)
      rm(pred, dpred)

      # Compile the chunked nearest-neighbor results
      nn[[1]]$nn.idx <- do.call(rbind, map(nn, "nn.idx"))
      nn[[1]]$nn.dists <- do.call(rbind, map(nn, "nn.dists"))
      nn <- nn[[1]]

      # If requested, remove the nearest match from 'nn'
      if (ignore_nearest) {
        nn$nn.idx <- nn$nn.idx[, -1, drop = FALSE]
        nn$nn.dists <- nn$nn.dists[, -1, drop = FALSE]
      }

      #----

      # Randomly select a similar observation from the donor
      # Optionally using inverse-distance probabilities for selection
      cat("-- Simulating fused values\n")
      if (max_dist > 0 | idw) {
        p <- nn$nn.dists
        p[nn$nn.idx == 0] <- NA
        if (idw) {
          p <- 1 / nn$nn.dists
          p[is.infinite(p)] <- min(p, na.rm = TRUE)  # Prevent infinite inverse distance
        } else {
          p[!is.na(p)] <- runif(sum(!is.na(p)))
        }
        p <- p / rowSums(p, na.rm = TRUE)
        for (j in 2:ncol(p)) p[, j] <- p[, j - 1] + p[, j]
        ptile <- runif(n = nrow(p))
        S <- rowSums(ptile > p, na.rm = TRUE) + 1L
      } else {
        # Simple case; randomly select a column
        S <- sample.int(n = ncol(nn$nn.idx), size = nrow(nn$nn.idx), replace = TRUE)
      }

      # Extract simulated values from donor observations
      ind <- nn$nn.idx[cbind(1:nrow(nn$nn.idx), S)]
      S <- dresp[ind, ]

      # If zeros are simulated separately, integrate them into 'S'
      if (any(zind)) {
        out <- data.table(rep(0, nrow(dmat)))
        set(out, i = which(!zind), j = "V1", value = S)
        S <- out
      }

    }

    # Add the simulated values to 'dmat' prior to next iteration
    S <- tomat(S)
    colnames(S) <- v
    dmat <- cbind(dmat, S)
    rm(S)

  }

  unlink(td)

  #---

  # Convert 'dmat' to desired output
  dmat <- dmat[, unlist(yord)]
  dmat <- data.table(as.matrix(dmat))

  # Ensure simulated variables are correct data type with appropriate labels/levels
  for (v in names(dmat)) {
    yclass <- meta$yclass[[v]]
    if ("factor" %in% yclass) {
      lev <- meta$ylevels[[v]]
      set(dmat, i = NULL, j = v, value = factor(lev[dmat[[v]] + 1], levels = lev, ordered = "ordered" %in% yclass))
    }
    if ("logical" %in% yclass) set(dmat, i = NULL, j = v, value = as.logical(dmat[[v]]))
    if ("integer" %in% yclass)  set(dmat, i = NULL, j = v, value = as.integer(dmat[[v]]))
  }

  return(dmat)

}

#------------------------------

# Fuse multiple implicates
#' @rdname fuse
#' @export
fuseM <- function(..., M, fun = fusionModel::fuse, seeds = NULL) {

  stopifnot({
    M >= 1 & M %% 1 == 0
  })

  # The lapply() wrapper is serial, but the underlying call to fuse() can be parallel via 'cores' argument
  out <- lapply(1:M, function(i) {
    cat("<< Generating implicate", i, "of", M, ">>\n")
    fun(..., seed = seeds[i])
  }) %>%
    bind_rows(.id = "M") %>%
    mutate(M = as.integer(M))

  return(out)

}
