#' Fuse variables to a recipient dataset
#'
#' @description
#' Fuse variables to a recipient dataset using a .fsn model produced by \code{\link{train}}. If multiple implicates are requested, output can be passed to \code{\link{analyze}}.
#'
#' @param data Data frame. Recipient dataset. All categorical variables should be factors and ordered whenever possible. Data types and levels are strictly validated against predictor variables defined in \code{model}.
#' @param file Character. Path to fusion model file (.fsn) generated by successful call to \code{\link{train}}.
#' @param k Integer. Number of nearest neighbors to identify among the donor observations.
#' @param M Integer. Number of implicates to simulate.
#' @param max_dist Numeric. Controls the maximum allowable distance when identifying up to \code{k} nearest neighbors. \code{max_dist = 0} (default) means no distance restriction is applied.
#' @param idw Logical. Should inverse distance weighting be used when randomly selecting a donor observation from the \code{k} nearest neighbors?
#' @param cores Integer. Number of cores used. LightGBM prediction is parallel-enabled on all systems, but kNN is only parallel on Unix (at present).
#' @param ignore_self Logical. If \code{TRUE}, the kNN step excludes "self matches" (i.e. row 1 in \code{data} cannot match with row 1 in the original donor. Only useful for validation exercises. Do not use otherwise.
#' @param margin Numeric. Safety margin used when estimating how many implicates can be processed in memory at once. Set higher if \code{fuse()} experiences a memory shortfall.
#' @param seed Set random seed if needed.
#'
#' @details For each record in \code{data}, the predicted conditional distribution values are used to identify the \code{k} most-similar observations in the original donor data along the same dimensions. One of the \code{k} nearest neighbors is randomly selected to donate its observed/"real" response value for the fusion variable(s) in question. The random selection uses inverse distance weighting if \code{idw = TRUE}; otherwise, the donor sample weights are used.
#' @details The \code{max_dist} value is a relative scaling factor. The search radius passed to \code{\link[RANN]{nn2}} is \code{max_dist * medDist}, where \code{medDist} is the median pairwise distance calculated among all of the donor observations. This approach allows \code{max_dist} to adjust the search radius in a consistent way across all fusion variables/blocks.
#'
#' @return For \code{fuse()}, a data frame with same number of rows as \code{data} and one column for each synthetic fusion variable. The order of the columns reflects the order in which they where fused.
#' @return If \code{M > 1}, a data frame with number of rows equal to \code{M * nrow(data)}. Integer column "M" indicates implicate assignment of each observation. Note that the ordering of recipient observations is consistent within implicates, so do not change the row order if using with \code{\link{analyze}}.
#'
#' @examples
#' # Build a fusion model using RECS microdata
#' # Note that "test_model.fsn" will be written to working directory
#' ?recs
#' fusion.vars <- c("electricity", "natural_gas", "aircon")
#' predictor.vars <- names(recs)[2:12]
#' train(data = recs, y = fusion.vars, x = predictor.vars, file = "test_model.fsn")
#'
#' # Generate single implicate of synthetic 'fusion.vars',
#' #  using original RECS data as the recipient
#' recipient <- recs[predictor.vars]
#' sim <- fuse(data = recipient, file = "test_model.fsn")
#' head(sim)
#'
#' # Calling fuse() again produces different results
#' sim <- fuse(data = recipient, file = "test_model.fsn")
#' head(sim)
#'
#' # Generate multiple implicates
#' sim <- fuse(data = recipient, file = "test_model.fsn", M = 5)
#' head(sim)
#' table(sim$M)
#'
#' # Test to confirm that 'ignore_self' works as intended
#' recs2 <- mutate(recs, electricity = jitter(electricity))
#' train(data = recs2, y = "electricity", x = predictor.vars, file = "test_model.fsn")
#'
#' sim1 <- fuse(data = recipient, file = "test_model.fsn", ignore_self = FALSE)
#' sim2 <- fuse(data = recipient, file = "test_model.fsn", ignore_self = TRUE)
#'
#' any(sim1 == recs2$electricity)
#' any(sim2 == recs2$electricity)
#'
#' @export

#---------------------

# Manual testing
# library(fusionModel)
# source("R/utils.R")

# #Example inputs
# data <- recs
# file <- "test_model.fsn"
# M <- 10

# Actual testing with CEI-ACS
# data <- fst::read_fst("~/Documents/Projects/fusionData/fusion/CEI/2015-2019/2019/CEI_2015-2019_2019_predict.fst")
# file <- "~/Documents/Projects/fusionData/fusion/CEI/2015-2019/2019/CEI_2015-2019_2019_model.fsn"
# M <- 10
# k <- 10
# max_dist = 0
# idw = FALSE
# cores = 3
# ignore_self <- FALSE
# margin = 0.2
# seed <- NULL

#---------------------

fuse <- function(data,
                 file,
                 k = 5,
                 M = 1,
                 max_dist = 0,
                 idw = FALSE,
                 cores = 1,
                 ignore_self = FALSE,
                 margin = 1.75,
                 seed = NULL) {

  t0 <- Sys.time()

  stopifnot(exprs = {
    is.data.frame(data)
    file.exists(file) & endsWith(file, ".fsn")
    k > 0 & k %% 1 == 0
    max_dist >= 0
    is.logical(idw)
    cores > 0 & cores %% 1 == 0 & cores <= parallel::detectCores(logical = FALSE)
    is.logical(ignore_self)
    margin >= 1
  })

  if (is.data.table(data)) data <- as.data.frame(data)
  if (!is.null(seed)) set.seed(seed)

  # Temporary directory to unzip to
  td <- tempfile()
  dir.create(td)

  # Names of files within the .fsn object
  fsn.files <- zip::zip_list(file)
  pfixes <- sort(unique(dirname(fsn.files$filename)))
  pfixes <- pfixes[pfixes != "."]

  # Load model metadata
  zip::unzip(zipfile = file, files = "metadata.rds", exdir = td)
  meta <- readRDS(file.path(td, "metadata.rds"))

  # Unzip ALL of the lightGBM model(s) to temp directory
  mods <- grep(pattern = glob2rx("*.txt"), x = fsn.files$filename, value = TRUE)
  zip::unzip(zipfile = file, files = mods, exdir = td)

  # Names, order, and 'blocks' of response variables
  yord <- meta$yorder

  # Check that necessary predictor variables are present
  xvars <- names(meta$xclass)
  miss <- setdiff(xvars, names(data))
  if (length(miss) > 0) stop("The following predictor variables are missing from 'data':\n", paste(miss, collapse = ", "))

  # Restrict 'data' to 'xvars'
  data <- data[xvars]

  # Check for appropriate class/type of predictor variables
  xclass <- meta$xclass
  xtest <- lapply(data, class)
  miss <- !purrr::map2_lgl(xclass, xtest, sameClass)
  if (any(miss)) stop("Incompatible data type for the following predictor variables:\n", paste(names(miss)[miss], collapse = ", "))

  # Check for appropriate levels of factor predictor variables
  xlevels <- meta$xlevels
  xtest <- lapply(subset(data, select = names(xlevels)), levels)
  miss <- !purrr::map2_lgl(xlevels, xtest, identical)
  if (any(miss)) stop("Incompatible levels for the following predictor variables\n", paste(names(miss)[miss], collapse = ", "))

  # Detect and impute any missing values in 'data'
  na.cols <- names(which(sapply(data, anyNA)))
  if (length(na.cols) > 0) {
    cat("Missing values imputed for the following variable(s):\n", paste(na.cols, collapse = ", "), "\n")
    for (j in na.cols) {
      x <- data[[j]]
      ind <- is.na(x)
      data[ind, j] <-  imputationValue(x, ind)
    }
  }

  #-----

  # Allocate columns in 'dmat' for the fusion variables
  N0 <- nrow(data)
  fmat <- matrix(data = NA_real_, nrow = N0, ncol = length(unlist(yord)), dimnames = list(NULL, unlist(yord)))
  fsize <- as.numeric(object.size(fmat)) / 1048576  # This would be more accurate if it accounted for factor/integer/logical y-variable classes in eventual output

  # Coerce 'data' to matrix compatible with input to LightGBM
  data <- cbind(as.data.table(data), fmat)
  setcolorder(data, meta$dnames)  # This ensures that 'dmat' has columns in correct/original order for purposes of LightGBM prediction
  dmat <- to_mat(data)

  # Ensure that 'dmat' is double precision
  # LightGBM enforces this internally, so it is more efficient to do it here
  if (storage.mode(dmat) != "double") storage.mode(dmat) <- "double"

  rm(data, fmat); gc()

  # Process multiple implicates at once...
  # Determine how may implicates can be processed at once, given available memory
  # fsize * M: the maximum size of the output/results matrix
  # dsize: initial size of the prediction data prior to expansion
  dsize <- as.numeric(object.size(dmat)) / 1048576  # Check size (Mb)
  mfree <- freeMemory() + dsize
  n <- floor(mfree / (fsize + dsize * margin))
  if (n <= 0) stop("Insufficient memory to store ", M, " implicates. 'M' must be smaller.")
  n <- min(n, M)
  nsteps <- ceiling(M / n)
  ind.final <- if (M %% n > 0) rep(seq.int(N0), M %% n) else NULL

  # Allocate the output data.table
  dout <- data.table()

  # Replicate the prediction matrix, if necessary
  if (n > 1) dmat <- dmat[rep(seq.int(N0), n), ]
  if (nsteps > 1) cat("Generating", M, "implicates in", nsteps, "chunk(s)\n")
  gc()

  #-----

  for (mstep in 1:nsteps) {

    if (nsteps > 1) cat("<< Processing implicate chunk", mstep, "of", nsteps, ">>\n")

    for (i in 1:length(pfixes)) {

      v <- meta$yorder[[i]]

      cat("Fusion step ", i, " of ", length(pfixes), ": ", paste(v, collapse = ", "), "\n", sep = "")

      block <- length(v) > 1

      # LightGBM predictor variables - NECESSARY?
      #xv <- meta$lgbpred[[i]]

      # Unzip the lightGBM model(s) to temp directory
      mods <- grep(pattern = glob2rx(paste0(pfixes[i], "*.txt")), x = fsn.files$filename, value = TRUE)

      # Row indices where to generate predictions
      # Modified, if necessary, in next code chunk when single continuous variables has a zero model
      zind <- rep(FALSE, nrow(dmat))

      #---

      # Simulate zero values for case of single continuous variable
      if (!block & meta$ytype[v[[1]]] == "continuous") {
        m <- grep("z\\.txt$", mods, value = TRUE)
        if (length(m)) {
          mod <- lightgbm::lgb.load(filename = file.path(td, m))
          p <- predict(object = mod, data = dmat, reshape = TRUE, params = list(num_threads = cores, predict_disable_shape_check = TRUE))
          zind <- p > runif(n = nrow(dmat)) # Row indices where a zero has been simulated (TRUE for a zero)
        }
        mods <- setdiff(mods, m)
      }

      #---

      # Load and make predictions for each model ('pred' object)
      cat("-- Predicting LightGBM models\n")
      pred <- data.table()
      for (m in mods) {
        y <- sub("_[^_]+$", "", basename(m))
        n <- sub(".txt", "", sub(paste0(y, "_"), "", basename(m), fixed = TRUE), fixed = TRUE)
        q <- substring(n, 1, 1) == "q"
        z <- substring(n, 1, 1) == "z"
        mod <- lightgbm::lgb.load(filename = file.path(td, m))  # TO DO: Load once at front...

        # Trying to speed-up this step
        # Removing 'xv' column subset to 'dmat' is much faster (something to do with how Matrix handles column subsetting)
        #p <- predict(object = mod, data = dmat[!zind, xv], reshape = TRUE, params = list(num_threads = cores))
        #p <- predict(object = mod, data = dmat[!zind, ], reshape = TRUE, params = list(num_threads = cores))

        # The row subsetting is slow, so only perform if necessary
        # if (any(zind)) {
        #   p <- predict(object = mod, data = dmat[!zind, ], reshape = TRUE, params = list(num_threads = cores, predict_disable_shape_check = TRUE))
        # } else {
        #   p <- predict(object = mod, data = dmat, reshape = TRUE, params = list(num_threads = cores, predict_disable_shape_check = TRUE))
        # }

        # It is much more memory efficient (and often faster) to simply predict for all observations and then subset for 'zind' afterwards
        p <- predict(object = mod, data = dmat, reshape = TRUE, params = list(num_threads = cores, predict_disable_shape_check = TRUE))
        if (any(zind)) p <- p[!zind]

        # To confirm that prediction is accurate when using 'predict_disable_shape_check = TRUE'
        # xv <- meta$lgbpred[[i]]
        # p2 <- predict(object = mod, data = dmat[!zind, xv], reshape = TRUE, params = list(num_threads = cores))
        # identical(p, p2)

        if (!is.matrix(p)) p <- matrix(p)
        set(pred, i = NULL, j = paste0(y, "_", n, if (q | z) NULL else 1:ncol(p)), value = as.data.table(p))
      }

      rm(p); gc()

      #---

      # Apply normalization, as necessary, to recipient conditional values
      norm.vars <- intersect(names(pred), names(meta$ycenter[[i]]))
      for (j in norm.vars) {
        ncenter <- meta$ycenter[[i]][j]
        nscale <- meta$yscale[[i]][j]
        if (nscale == 0) {
          set(pred, i = NULL, j = j, value = NULL)
        } else {
          val <- normalize(pred[[j]], center = ncenter, scale = nscale)
          set(pred, i = NULL, j = j, value = val)
        }
      }

      #---

      # Simulate values for case of single categorical variable
      if (!block & meta$ytype[v[[1]]] != "continuous") {
        cat("-- Simulating fused values\n")
        ptile <- runif(n = nrow(pred))
        S <- if (ncol(pred) > 1) {
          for (j in 2:ncol(pred)) set(pred, i = NULL, j = j, value = pred[[j - 1]] + pred[[j]])
          for (j in 1:ncol(pred)) set(pred, i = NULL, j = j, value = ptile > pred[[j]])
          rowSums(pred)  # First level receives value of zero
        } else {
          pred[[1]] > ptile  # The binary case
        }
      }

      #---

      if (block | meta$ytype[[v[1]]] == "continuous") {

        # Get the donor conditional prediction and response variables
        x <- paste0(pfixes[i], "/donor.fst")
        zip::unzip(zipfile = file, files = x, exdir = td)  # # TO DO: Load once at front...
        dtemp <- fst::read_fst(file.path(td, x), as.data.table = TRUE) # TO DO: Load once at front?
        dresp <- select(dtemp, any_of(v))
        dwgt <- dtemp$W..
        drow <- dtemp$R..
        dpred <- select(dtemp, -any_of(c(v, "W..", "R..")))
        pred <- select(pred, all_of(names(dpred)))  # This ensures 'pred' and 'dpred' have identical columns
        rm(dtemp)

        # Apply the column weights to 'pred'
        for (j in names(pred)) set(pred, i = NULL, j = j, value = pred[[j]] * meta$colweight[[i]][[j]])

        # Ensure none of the normalized conditional values fall outside the range observed in 'dpred'
        for (j in names(pred)) set(pred, i = NULL, j = j, value = pmin(pred[[j]], max(dpred[[j]])))
        for (j in names(pred)) set(pred, i = NULL, j = j, value = pmax(pred[[j]], min(dpred[[j]])))

        #----

        # Perform K-nearest-neighbor search
        # By default, this uses parallel processing as determined by 'cores' argument
        # The 'query' argument to nn2() is split into 'cores' chunks and processed in parallel
        # This could generate memory issues if 'pred' is too large
        # Safer approach would be to detect available memory and set number of chunks dynamically
        # NOTE: The 'eps' error bound in nn2() is set dynamically (speed/precision tradeoff)
        #  This appears to be a reasonable setting based on limited testing, but not definitive

        getNN <- function(x) {

          mdist <- meta$meddist[[i]]

          nn <- RANN::nn2(data = dpred,
                          query = x,
                          k = k + ignore_self,
                          searchtype = ifelse(max_dist == 0, "standard", "radius"),
                          radius = mdist * max_dist,
                          eps = sqrt(mdist))

          # Apply potential fix-up for cases where there is no nearest neighbor within the specified search radius
          if (max_dist > 0) {
            fix <- which(nn$nn.idx[, 1] == 0)
            if (length(fix) > 0) {
              nn.fix <- RANN::nn2(data = dpred,
                                  query = x[fix, ],
                                  k = 1 + ignore_self,  # Returns single nearest-neighbor
                                  searchtype = "standard",
                                  eps = sqrt(mdist))
              nn$nn.idx[fix, 1] <- nn.fix$nn.idx
              nn$nn.dists[fix, 1] <- nn.fix$nn.dists
              rm(fix, nn.fix)
            }
          }
          return(nn)

        }

        # Attempt PCA to reduce dimensionality prior to nn2?
        # https://github.com/ummel/fusionModel/issues/29
        # pca.fit <- prcomp(x = dpred, retx = TRUE, center = FALSE, scale. = FALSE, rank. = 8)
        # dpred2 <- pca.fit$x
        # pred2 <- predict(pca.fit, newdata = pred)

        # Split 'pred' into chunks and process in parallel -- could this be more memory efficient?
        # Prevents very small chunks (too much overhead); enforces serial processing on Windows
        cat("-- Finding nearest neighbors\n")
        pred <- split(pred, f = rep(1:cores, each = ceiling(nrow(pred) / cores))[1:nrow(pred)])
        nn <- parallel::mclapply(pred, getNN, mc.cores = cores)

        rm(pred, dpred); gc()

        # Compile the chunked nearest-neighbor results
        nn[[1]]$nn.idx <- do.call(rbind, purrr::map(nn, "nn.idx"))
        nn[[1]]$nn.dists <- do.call(rbind, purrr::map(nn, "nn.dists"))
        nn <- nn[[1]]

        # If requested, exclude self-matches in 'nn'
        # Note that 'ignore_self = TRUE' only makes sense if nrow(nn$nn.idx) == nrow(dpred) - CORRECT?
        # TO DO: Need the original row number in 'dpred'?
        if (ignore_self) {
          if (is.null(drow)) drow <- seq_along(dwgt)
          self.refer <- drow[nn$nn.idx] == which(!zind)
          nn$nn.idx[which(self.refer)] <- 0
        }

        # Create 'p' matrix with selection weights for each nearest neighbor
        nn$nn.idx[nn$nn.idx == 0] <- NA
        nn$nn.dists[is.na(nn$nn.idx)] <- NA
        p <- nn$nn.dists
        if (idw) {
          p <- 1 / p
          p[is.infinite(p)] <- min(p, na.rm = TRUE) # Prevent infinite inverse distance
        } else {
          p[] <- dwgt[nn$nn.idx]  # Selection weights given by "W.." (donor original sample weights)
        }

        # Randomly sample a column from 'p', using the provided selection weights
        # This is convoluted but probably the fastest way to randomly select columns using selecton weights and accomodating excluding/prohibited neighbors (i.e. ignore_self)
        cat("-- Simulating fused values\n")
        if (ignore_self) p[!matrixStats::rowAnyNAs(p), ncol(p)] <- NA  # Ignore the extra column, if there is no self-referral in the row
        na.ind <- is.na(p)
        p[na.ind] <- 0
        p <- p / matrixStats::rowSums2(p)
        p <- matrixStats::rowCumsums(p)
        p[na.ind] <- NA
        ptile <- runif(n = nrow(p))
        S <- matrixStats::rowSums2(ptile > p, na.rm = TRUE) + 1L
        adj <- which(is.na(matrixStats::rowCollapse(p, S)))
        rm(p, na.ind, ptile); gc()
        S[adj] <- S[adj] + 1L

        # Extract simulated values from donor observations
        ind <- matrixStats::rowCollapse(nn$nn.idx, S)
        stopifnot(!anyNA(ind))  # Safety check
        rm(nn, S); gc()
        S <- dresp[ind, ]

        # NOT USED: Simple case; randomly select a column
        # This only works if there are no selection weights
        #S <- sample.int(n = ncol(nn$nn.idx), size = nrow(nn$nn.idx), replace = TRUE)

        # If zeros are simulated separately, integrate them into 'S'
        # This is only relevant when fusing a single continuous variable
        if (any(zind)) {
          out <- data.table(rep(0, nrow(dmat)))
          setnames(out, v)
          set(out, i = which(!zind), j = v, value = S)
          S <- out
          rm(out); gc()
        }

      }

      # Add the simulated values to 'dmat' prior to next iteration
      S <- as.matrix(S)
      dmat[, v] <- S
      rm(S); gc()

    }

    #---

    # All blocks are processed, extract the fusion variable simulated values
    # Convert 'dmat' to desired output, retaining only the fusion variables

    dtemp <- as.data.table(dmat[, unlist(yord), drop = FALSE])

    # Ensure simulated variables are correct data type with appropriate labels/levels
    # This also reduces the storage memory if any integer or logical columns are present
    for (v in names(dtemp)) {
      yclass <- meta$yclass[[v]]
      if ("factor" %in% yclass) {
        lev <- meta$ylevels[[v]]
        set(dtemp, i = NULL, j = v, value = factor(lev[dtemp[[v]] + 1], levels = lev, ordered = "ordered" %in% yclass))
      }
      if ("logical" %in% yclass) set(dtemp, i = NULL, j = v, value = as.logical(dtemp[[v]]))
      if ("integer" %in% yclass)  set(dtemp, i = NULL, j = v, value = as.integer(dtemp[[v]]))
    }

    # Add the output simulated values to 'dout'
    dout <- rbind(dout, dtemp)
    rm(dtemp); gc()

    # Update 'dmat' prior to next iteration
    if (mstep < nsteps) {
      dmat[, unlist(yord)] <- NA_real_
      if (mstep == (nsteps - 1) & !is.null(ind.final)) {
        dmat <- dmat[ind.final, ]
      }
    } else {
      rm(dmat); gc()
    }

  }

  #-----

  # Add 'M' column to 'dout' indicating the implicate (only if multiple implicates)
  if (M > 1) {
    set(dout, i = NULL, j = "M", value = rep(1:M, each = N0))
    setcolorder(dout, "M")
  }

  # Remove temporary directory
  unlink(td)

  #-----

  # Report processing time
  tout <- difftime(Sys.time(), t0)
  cat("Total processing time:", signif(as.numeric(tout), 3), attr(tout, "units"), "\n", sep = " ")

  return(dout)

}
