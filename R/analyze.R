#' Analyze synthetic data to calculate point estimates and uncertainty
#'
#' @description
#' Calculation of point estimates and associated standard errors for analyses using synthetic microdata. Analysis is currently limited to means, totals, and proportions, (optionally) calculated for population subgroups. Plan to extend to regression model coefficients and custom microsimulation analyses in future. User must supply a list of synthetic implicates (typically generated by \link{fuseM}) and "static" (non-synthetic) variables required by the analysis, including sampling and replicate weights.
#'
#' @param formula A formula expression describing the desired analysis. Example: \code{var1 ~ 1} requests mean and totals if \code{var1} is numeric or proportions if \code{var1} is a factor.
#' @param by Character. Column name(s) in \code{synthetic} or \code{static} (typically factors) that collectively define the population subgroups to analyze.
#' @param synthetic List of data frames containing synthetic implicates, each with the same number of rows as \code{static}. Typically generated by \link{fuseM}.
#' @param static Data frame containing static (non-synthetic) variables with the same number of rows as \code{synthetic}. These data are "static" since they do not vary across implicates. At a minimum, this must include a column named \code{"weight"} giving the primary sampling weights and at least two columns of replicate weights denoted \code{"rep_1"}, \code{"rep_2"}, etc.
#' @param donor.N Number of (un-weighted) observations in the original "donor" data used to generate the implicates.
#' @param syn.N Number of (un-weighted) observations in the "recipient" data. Usually \code{nrow(static)}, the default.
#' @param var.scale Value by which to scale the standard variance calculated across replicate weights. This is specified by the survey design. \code{var.scale = 4} (the default) for both RECS and ACS.
#' @param cores Integer. Number of cores used for parallel operations.
#'
#' @details The requested analysis is performed separately for each implicate. The final point estimate is the mean estimate across implicates. The final standard error is pooled across implicates using Reiter's pooling rules for partially synthetic data (Reiter, 2003). The estimate and standard error of each implicate is calculated using the provided observation weights. Calculations utilize custom code leveraging \code{\link[data.table]{data.table}} operatons for speed and memory efficiency.
#' @details The within-implicate variance is calculated around the mean of the replicates, rather than the point estimate. This is equivalent to \code{mse = FALSE} in \code{\link[survey]{svrepdesign}}. While this is not necessarily correct for all surveys, the effect on final errors is negligible.
#'
#'
#'
#' @return A tibble reporting results for the \code{response} variable across population subgroups of \code{by}. The returned quantities are:
#' @return \describe{
#'   \item{estimate}{Point estimate. Mean of estimates across the implicates.}
#'   \item{std_error}{Pooled standard error of the point estimate.}
#'   \item{lower_ci}{Lower bound of 95% confidence interval.}
#'   \item{upper_ci}{Upper bound of 95% confidence interval.}
#'   \item{degf}{Degrees of freedom of t-distribution if calculating custom confidence intervals.}
#'   }
#'
#' @references
#' Reiter, J.P. (2003). Inference for Partially Synthetic,
#' Public Use Microdata Sets. \emph{Survey Methodology}, \bold{29}, 181-189.
#'
#' @examples
#' # Build a fusion model using RECS microdata
#' fusion.vars <- c("electricity", "natural_gas", "aircon")
#' predictor.vars <- names(recs)[2:12]
#' fit <- train(data = recs, y = fusion.vars, x = predictor.vars)
#'
#' # Generate 5 implicates of synthetic 'fusion.vars',
#' #  using original RECS data as the recipient
#' sim <- fuseM(data = recs, train.object = fit, M = 5)
#'
#' # Analyze electricity consumption, by climate zone
#' result <- analyze(formula = electricity ~ 1,
#'                   by = "climate",
#'                   synthetic = sim,
#'                   static = recs,
#'                   donor.N = nrow(recs))
#'
#' # Analyze air conditioning technology, by race
#' result <- analyze(formula = aircon ~ 1,
#'                   by = "race",
#'                   synthetic = sim,
#'                   static = recs,
#'                   donor.N = nrow(recs))
#' @export
#-----

analyze <- function(formula,
                    by,
                    synthetic,
                    static,
                    donor.N,
                    syn.N = nrow(static),
                    var.scale = 4,
                    cores = 1) {

  if (missing(by)) by <- NULL

  stopifnot({
    class(formula) == "formula"
    is.null(by) | is.character(by)
    is.list(synthetic)
    length(synthetic) > 1
    is.data.frame(static)
    donor.N > 0 & donor.N %% 1 == 0
    syn.N > 0 & syn.N %% 1 == 0
    as.numeric(var.scale)
    cores > 0 & cores %% 1 == 0
    all(sapply(synthetic, nrow) == nrow(static))
    all(c("weight", "rep_1", "rep_2") %in% names(static))
  })

  # Variance adjustment factor based on synthetic sample size relative to original data
  # This is used to inflate the within-imputation variance when syn.N and donor.N are different
  #vaf <- syn.N / donor.N

  # Determine variables used within 'formula'; will throw error if all required variables cannot be located
  dtemp <- cbind(synthetic[[1]][1, ], static[1, ])
  fvars <- names(get_all_vars(formula, data = dtemp))

  # Retain only necessary columns in the input data
  # Conversion of numeric to double is to prevent any data type issues with data.table
  synthetic <- synthetic %>%
    map(~ .x %>%
          select(any_of(c(fvars, by))) %>%
          mutate_if(is.numeric, as.double) %>%
          as.data.table())

  static <- static %>%
    select(any_of(setdiff(c(fvars, by), names(synthetic[[1]]))), weight, starts_with("rep_")) %>%
    mutate_if(is.numeric, as.double) %>%
    as.data.table()

  # Names of the replicate weights in 'static'
  # Note that the primary weight variable is assumed to be called "weight"
  wrep <- names(select(static, starts_with("rep_")))

  # Calculation type
  rvar <- fvars[1]  # The "response" variable
  type <- ifelse("factor" %in% class(dtemp[[rvar]]) | is.logical(dtemp[[rvar]]), "proportion", "mean")
  if (type == "factor" & length(fvars) > 1) stop("Factor variable not allowed as dependent variable in regression models")
  type <- ifelse(length(fvars) > 1, "model", type)

  #---------

  # Apply the following function to each subset of 'dt' defined by key(dt)
  #x <- dt[climate == "IECC climate zones 3B-4B"]
  calcMean <- function(x) {

    # Point estimate of "total"
    pe.tot <- sum(x[[rvar]] * x$weight)

    # Point estimate of "mean"
    pe.mu <- pe.tot / sum(x$weight)

    # Standard error across implicates
    # This calculation uses mean(tot) instead of 'pe.tot' for calculation of variance
    # This is equivalent to setting mse = FALSE in svrepdesign()
    reptot <- paste0(wrep, ".t")
    tot <- colSums(x[, ..reptot])  # Sum total for each replicate
    se.tot <- sqrt(var.scale * sum((tot - mean(tot)) ^ 2) / length(tot))  # Scaling factor at front (4 for both RECS and ACS)
    mu <- tot / colSums(x[, ..wrep])  # Mean values across replicates
    se.mu <- sqrt(var.scale * sum((mu - mean(mu)) ^ 2) / length(tot))  # Scaling factor at front (4 for both RECS and ACS)

    out <- tibble(metric = c("mean", "total"),
                  estimate = c(pe.mu, pe.tot),  # Mean and Total point estimates
                  se = c(se.mu, se.tot))

    return(out)

  }

  #---------

  # Calculate estimates and standard errors for each implicate
  imp.estimates <- pbapply::pblapply(synthetic, FUN = function(syn.data) {

    # Create implicate-specific data.table
    dt <- cbind(syn.data, static)

    if (type == "mean") {

      # Calculate totals
      for (j in wrep) set(dt, i = NULL, j = paste0(j, ".t"), value = dt[[j]] * dt[[rvar]])

      # Apply calcEst(), possibly by group
      out <- if (is.null(by)) {
        calcMean(dt)
      } else {
        setkeyv(dt, by)
        dt[, calcMean(.SD), by = key(dt)]
      }

    }

    if (type == "proportion") {

      rlev <- levels(dt[[rvar]])
      dt <- dt[, lapply(.SD, sum), .SDcols = c("weight", wrep), by = c(by, rvar)]
      f <- function(x) x / sum(x)
      dt <- dt[, c("weight", wrep) := lapply(.SD, f), .SDcols = c("weight", wrep), by = by]

      out <- dt %>%
        mutate(se = sqrt(var.scale * rowSums((dt[, ..wrep] - rowMeans(dt[, ..wrep])) ^ 2) / length(wrep))) %>%
        rename(estimate = weight,
               level = !!rvar)

    }

    # Assemble output
    out <- out %>%
      mutate(response = rvar) %>%
      select(all_of(by), response, any_of(c("metric", "level")), estimate, se)

    return(out)

  }, cl = cores) %>%
    bind_rows(.id = "M")

  #---------

  # Identify which columns in 'imp.estimates' to group by below
  grp <- intersect(names(imp.estimates), c("response", "metric", "level", "term"))
  for (v in grp) imp.estimates[[v]] <- factor(imp.estimates[[v]], levels = unique(imp.estimates[[v]]))

  # Calculate pooled estimates and standard errors using Reiter (2003) pooling rules for synthetic data
  # The calculations are taken from the mice package: https://github.com/amices/mice/blob/master/R/pool.R
  # Analogous calculations are found in the synthpop package: https://github.com/cran/synthpop/blob/master/R/methods.syn.r
  # See ?summary.fit.synds
  result <- imp.estimates %>%
    group_by_at(c(by, grp)) %>%
    summarize(
      m = n(),  # Number of implicates for which there are data
      qbar = mean(.data$estimate),  # Pooled complete data estimate
      ubar = mean(.data$se ^ 2) * syn.N / donor.N,  # Within-imputation variance of estimate; ADJUSTED for sample size
      b = var(.data$estimate),  # Between-imputation variance of estimate
      t = ubar + (1 / m) * b,  # Total variance, of estimate; the square root of 't' is the standard error (see below)
      df = (m - 1) * (1 + (ubar / (b / m))) ^ 2,  # Degrees of freedom of $t$-statistic
      .groups = "drop"
    ) %>%
    rename(
      estimate = qbar,
      degf = df
    ) %>%
    mutate(
      std_error = sqrt(t),  # Standard error
      lower_ci = estimate + qt(0.025, df = degf) * std_error, # 95% confidence interval lower bound
      upper_ci = estimate + qt(0.975, df = degf) * std_error  # 95% confidence interval upper bound
    ) %>%
    select(all_of(c(by, grp)), estimate, std_error, lower_ci, upper_ci, degf)

  #---------

  # Constrain CI's to (0, 1) when proportions are being reported
  if ("level" %in% names(result)) {
    result <- result %>%
      mutate(lower_ci = pmax(0, lower_ci),
             upper_ci = pmin(1, upper_ci))
  }

  # Constraint lower CI to zero if the response variable is strictly positive
  if ("metric" %in% names(result)) {
    # Is the response variable strictly positive?
    pos <- all(!map_lgl(synthetic, ~ any(.x[[rvar]] < 0))) & !any(static[[rvar]] < 0)
    if (pos) result <- mutate(result, lower_ci = pmax(0, lower_ci))
  }

  return(result)

}

#--------

# # DEPRECATED -- but may want to pull bits of this code in future
# # # Function to calculate estimate and standard error, given a formula, data frame, and specified type of calculation
# # # 'type' can be "mean", "proportion", or "model"
# estCalc <- function(formula, data, type) {
#
#   if (type == "mean") {
#     x <- data[[1]]
#     w <- data$weight
#     out <- tibble(response = names(data)[1],
#                   metric = c("mean", "total"),
#                   estimate = wmu(x, w) * c(1, sum(w)),  # Mean and Total
#                   std.error = (wsd(x, w) / sqrt(nrow(data))) * c(1, sum(w)),
#                   df = nrow(data) - 1L)
#   }
#
#   if (type == "proportion") {
#     p <- xtabs(as.formula(paste0("weight~", names(data)[1])), data = data)
#     p <- as.numeric(p / sum(p))
#     out <- tibble(response = names(data)[1],
#                   level = if (is.logical(data[[1]])) c(FALSE, TRUE) else levels(data[[1]]),
#                   estimate = p,
#                   std.error = sqrt(estimate * (1 - estimate) / nrow(data)),
#                   df = nrow(data) - 1L)
#   }
#
#   # tidy generics: https://cran.r-project.org/web/packages/generics/readme/README.html
#   if (type == "model") {
#     out <- suppressWarnings(tidyGLM(glm(formula = formula, data = data, weights = weight))) %>%
#       mutate(response = names(data)[1]) %>%
#       select(response, term, estimate, std.error, df)
#   }
#
#   return(out)
#
# }
#
# #--------
#
# # Function to return "tidied" glm() results
# # Based on function in broom package: https://github.com/tidymodels/broom/blob/main/R/stats-glm-tidiers.R
#
# tidyGLM <- function(x) {
#   xsummary <- summary(x)
#   ret <- as_tibble(xsummary$coefficients, rownames = "term")
#   colnames(ret) <- c("term", "estimate", "std.error", "statistic", "p.value")
#   ret$df <- xsummary$df.residual  # Add degrees of freedom
#   coefs <- tibble::enframe(stats::coef(x), name = "term", value = "estimate")
#   ret <- left_join(coefs, ret, by = c("term", "estimate"))
#   return(ret)
# }
