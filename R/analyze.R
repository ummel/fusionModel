#' Analyze synthetic data
#'
#' @description
#' Calculation of point estimates and associated standard errors for analyses using fused/synthetic microdata. Can calculate means, proportions, and linear regression coefficients, optionally performed across population subgroups.
#'
#' @param formula A formula describing the desired analysis. Example: \code{var1 ~ 1} requests mean and total if \code{var1} is numeric or proportions for individual levels if \code{var1} is a factor. Regression model formulas can be specified as in \code{\link[stats]{glm}}.
#' @param implicates Data frame containing implicates of synthetic (fused) variables. Typically generated by \link{fuseM}. The implicates should be row-stacked and identified by integer column "M".
#' @param donor.N Number of (un-weighted) observations in the original "donor" data used to generate the implicates.
#' @param sample_weights Vector of primary sampling weights. If absent, equal weights are assumed.
#' @param replicate_weights Optional data frame where each column is a set of survey replicate weights. \code{nrow(replicate_weights)} should equal \code{length(sample_weights)}. If \code{NULL}, standard errors are calculated without using replicate weights.
#' @param static Optional data frame containing static (non-synthetic) variables that do not vary across implicates. These are variables either required by \code{formula} or \code{by}.
#' @param by Character. Optional column name(s) in \code{implicates} or \code{static} (typically factors) that collectively define the population subgroups across which the analysis will be performed. If \code{NULL}, analysis is performed on the whole sample.
#' @param var.scale Scalar by which to scale the standard variance in the presence of replicate weights. This is determined by the survey design. The default (\code{var.scale = 4}) is appropriate for both RECS and ACS.
#' @param cores Integer. Number of cores used. Only applicable on Unix systems.
#'
#' @details At a minimum, the user must supply synthetic implicates (typically generated by \link{fuseM}). Inputs are checked for consistent dimensions.
#' @details If \code{implicates} contains only a single implicate and \code{replicate_weights = NULL}, the "typical" standard error is returned with a warning to make sure the user is aware of the situation.
#' @details Estimates and standard errors for the requested analysis are calculated separately for each implicate. The final point estimate is the mean estimate across implicates. The final standard error is the pooled SE across implicates, calculated using Rubin's pooling rules (1987) with a finite population adjustment of the degrees of freedom (Barnard and Rubin 1999).
#' @details When replicate weights are provided, the standard errors of each implicate are calculated via the variance of estimates across replicates. Calculations leverage \code{\link[data.table]{data.table}} operations for speed and memory efficiency. The within-implicate variance is calculated around the point estimate (rather than around the mean of the replicates). This is equivalent to \code{mse = TRUE} in \code{\link[survey]{svrepdesign}}. This seems to be the appropriate method for most surveys.
#' @details If replicate weights are NOT provided, the standard errors of each implicate are calculated using variance within the implicate. For means, the ratio variance approximation of Cochran (1977) is used, as this is known to be a good approximation of bootstrapped SE's for weighted means (Gatz and Smith 1995). For proportions, a generalization of the unweighted SE formula is used (\href{https://stats.stackexchange.com/questions/159204/how-to-calculate-the-standard-error-of-a-proportion-using-weighted-data}{see here}). For regression coefficients, the standard error is calculated by \code{\link[stats]{summary.glm}}.
#'
#' @return A tibble reporting analysis results, possibly across population subgroups defined in \code{by}. The returned quantities include:
#' @return \describe{
#'   \item{estimate}{Point estimate. Mean of estimates across implicates.}
#'   \item{std_error}{Standard error of the point estimate.}
#'   \item{lower_ci}{Lower bound of 95% confidence interval.}
#'   \item{upper_ci}{Upper bound of 95% confidence interval.}
#'   \item{statistic}{Test statistic; z-value for proportions and t-value otherwise.}
#'   \item{pvalue}{p-value of the test statistic, rounded to five decimals.}
#'   \item{degf}{Degrees of freedom. Zero for proportions. \code{N - 1} if a single implicate is provided.}
#'   \item{nobs}{Mean number of observations across the implicates.}
#'   }
#'
#' @references Barnard, J., & Rubin, D.B. (1999). Small-sample degrees of freedom with multiple imputation. \emph{Biometrika, 86}, 948-955.
#' @references Cochran, W. G. (1977). \emph{Sampling Techniques} (3rd Edition). Wiley, New York.
#' @references Gatz, D.F., and Smith, L. (1995). The Standard Error of a Weighted Mean Concentration — I. Bootstrapping vs Other Methods. \emph{Atmospheric Environment}, vol. 29, no. 11, 1185–1193.
#' @references Rubin, D.B. (1987). \emph{Multiple imputation for nonresponse in surveys}. Hoboken, NJ: Wiley.
#'
#' @examples
#' # Build a fusion model using RECS microdata
#' fusion.vars <- c("electricity", "natural_gas", "aircon")
#' predictor.vars <- names(recs)[2:12]
#' train(data = recs, y = fusion.vars, x = predictor.vars, file = "test_model.fsn")
#'
#' # Generate 10 implicates of the 'fusion.vars' using original RECS as the recipient
#' recipient <- recs
#' sim <- fuseM(data = recs, file = "test_model.fsn", M = 10)
#' head(sim)
#'
#' #---------
#'
#' # Analyze simulated electricity consumption,
#' # by climate zone, without replicate weights
#' result1 <- analyze(formula = electricity ~ 1,
#'                   implicates = sim,
#'                   sample_weights = recipient$weight,
#'                   replicate_weights = NULL,
#'                   static = recipient,
#'                   by = "climate",
#'                   donor.N = nrow(recs))
#'
#' # Analyze simulated electricity consumption,
#' # by climate zone, WITH replicate weights
#' result2 <- analyze(formula = electricity ~ 1,
#'                    implicates = sim,
#'                    sample_weights = recipient$weight,
#'                    replicate_weights = recipient[startsWith(names(recipient), "rep_")],
#'                    static = recipient,
#'                    by = "climate",
#'                    donor.N = nrow(recipient))
#'
#' # Helper function for comparison plots
#' pfun <- function(x, y) {plot(x, y); abline(0, 1, lty = 2)}
#'
#' # Inclusion of replicate weights does not affect estimates, but it does
#' # increase standard errors considerably due to RECS small sample size
#' pfun(result1$estimate, result2$estimate)
#' pfun(result1$std_error, result2$std_error)
#'
#' #---------
#'
#' # Conduct the same analyses using the original RECS data
#' # This generates a warning, since only an (implicit) single implicate is provided
#'
#' # Without replicate weights
#' orig1 <- analyze(formula = electricity ~ 1,
#'                 implicates = recs,
#'                 sample_weights = recs$weight,
#'                 replicate_weights = NULL,
#'                 static = recs,
#'                 by = "climate",
#'                 donor.N = nrow(recs))
#'
#' # And WITH replicate weights
#' orig2 <- analyze(formula = electricity ~ 1,
#'                  implicates = recs,
#'                  sample_weights = recs$weight,
#'                  replicate_weights = recs[startsWith(names(recs), "rep_")],
#'                  static = recs,
#'                  by = "climate",
#'                  donor.N = nrow(recs))
#'
#' #---------
#'
#' # Other examples of analyze() syntax and usage
#'
#' # Proportions of air conditioning technologies, by race, with replicate weights
#' result <- analyze(formula = aircon ~ 1,
#'                   implicates = sim,
#'                   sample_weights = recipient$weight,
#'                   replicate_weights = select(recipient, starts_with("rep_")),
#'                   static = recipient,
#'                   by = "race",
#'                   donor.N = nrow(recipient))
#'
#' # Linear regression, full sample, without replicate weights
#' result <- analyze(formula = electricity ~ income + aircon + climate,
#'                   implicates = sim,
#'                   sample_weights = recipient$weight,
#'                   replicate_weights = NULL,
#'                   static = recipient,
#'                   by = NULL,
#'                   donor.N = nrow(recipient))
#'
#' # Natural gas means and totals, by heating technology and climate, without replicate weights
#' result <- analyze(formula = natural_gas ~ 1,
#'                   implicates = sim,
#'                   sample_weights = recipient$weight,
#'                   replicate_weights = NULL,
#'                   static = recipient,
#'                   by = c("heat_type", "climate"),
#'                   donor.N = nrow(recipient))
#'
#' #---------
#'
#' # Testing replicate weight calculations
#' # Confirm that analyze() can reproduce results from 'survey' package
#'
#' # Create 'survey' package RECS survey design object
#' recs.design <- survey::svrepdesign(
#'   data = recs,
#'   weights = recs$weight,
#'   repweights = select(recs, starts_with("rep_")),
#'   type = "Fay",
#'   rho = 0.5,
#'   mse = TRUE
#' )
#'
#'  # Perform checks for mean and total electricity, by climate
#' check <- survey::svyby(electricity ~ 1, by = recs$climate, recs.design, FUN = survey::svymean)
#' test <- analyze(formula = electricity ~ 1,
#'                 implicates = recs,
#'                 sample_weights = recs$weight,
#'                 replicate_weights = select(recs, starts_with("rep_")),
#'                 by = "climate",
#'                 donor.N = nrow(recs))
#'
#' # Confirm that results are identical
#' all.equal(check$electricity, filter(test, metric == "mean")$estimate)
#' all.equal(check$se, filter(test, metric == "mean")$std_error)
#'
#' @export

#-----

analyze <- function(formula,
                    implicates,
                    donor.N,
                    sample_weights = NULL,
                    replicate_weights = NULL,
                    static = NULL,
                    by = NULL,
                    var.scale = 4,
                    cores = 1) {

  stopifnot({
    class(formula) == "formula"
    is.data.frame(implicates)
    donor.N > 0 & donor.N %% 1 == 0
    is.null(sample_weights) | is.numeric(sample_weights)
    is.null(replicate_weights) | (is.data.frame(replicate_weights) & ncol(replicate_weights) >= 2 & all(sapply(replicate_weights, is.numeric)))
    is.null(static) | is.data.frame(static)
    is.null(by) | is.character(by)
    is.numeric(var.scale)
    cores > 0 & cores %% 1 == 0
  })

  # Add 'M' columns to 'implicates' if it does not exist; assume there is only 1 implicate in this case
  # If already present, ensure the values of M are dense-ranked from 1 onward
  if ("M" %in% names(implicates)) {
    implicates$M <- dplyr::dense_rank(implicates$M)
  } else {
    implicates$M <- 1L
    warning("Assuming 'implicates' contains a single implicate (M = 1)")
  }

  # Check that other inputs' dimensions are consistent with 'implicates'
  mcnt <- max(implicates$M)
  N <- nrow(implicates) / mcnt
  tb <- table(implicates$M)
  stopifnot(all(names(tb) %in% seq_len(mcnt)))
  stopifnot(all(tb == N))
  if (is.null(sample_weights)) sample_weights = rep(1L, N)
  if (length(sample_weights) != N) ("'sample_weights' is incorrect length")
  if (!is.null(replicate_weights) & any(nrow(replicate_weights) != N)) ("'replicate_weights' has incorrect number of rows")
  if (!is.null(static) & any(nrow(static) != N)) ("'static' has incorrect number of rows")

  # If only a single implicate is provided and no replicate weights, issue appropriate warning
  if (mcnt == 1 & is.null(replicate_weights)) warning("Single implicate and no replicate weights provided.")
  # if (mcnt < 5) warning("Only ", mcnt, " implicates provided. Reliability of results increases with the number of implicates.")

  # Determine variables used within 'formula'
  # Will throw error if all required variables cannot be located
  dtemp <- if (is.null(static)) {
    implicates[1, ]
  } else {
    cbind(implicates[1, ], static[1, ])
  }
  fvars <- names(get_all_vars(formula, data = dtemp))
  if (any(by %in% fvars)) stop("'by' variables are not allowed in 'formula'")

  # Retain only necessary columns in 'implicates'
  # Conversion of numeric to double is to prevent any data type issues with data.table
  implicates <- implicates %>%
    select(any_of(c("M", fvars, by))) %>%
    mutate_if(is.numeric, as.double) %>%
    as.data.table() %>%
    setkey(M)

  # Retain only necessary columns in the static data
  # NOTE: Adds the 'sample_weights' and 'replicate_weights' to 'static' so they are carried in a single data.table
  # Note that this results in prioritization of 'implicates' when a variable is requested that is ALSO in 'static'; see below: cbind(implicates[M == m, ], static)
  static <- if (is.null(static)) matrix(nrow = N, ncol = 0) else static[setdiff(c(fvars, by), names(implicates))]
  static <- static %>%
    data.frame(cbind(.WGT = sample_weights, replicate_weights)) %>%
    mutate_if(is.numeric, as.double) %>%
    as.data.table()

  # Names of the replicate weights in 'replicate_weights'
  # Note that the primary weight variable is assumed to be called "weight"
  wrep <- names(replicate_weights)

  # Set 'by' to placeholder if not specified
  if (is.null(by)) {
    by <- "by..placeholder"
    static$by..placeholder <- 1L
  }

  # Determine calculation type and response variable
  rvar <- fvars[1]  # The "response" variable
  type <- ifelse("factor" %in% class(dtemp[[rvar]]) | is.logical(dtemp[[rvar]]), "proportion", "mean")
  if (type == "factor" & length(fvars) > 1) stop("Factor variable not allowed as dependent variable in regression models")
  type <- ifelse(length(fvars) > 1, "regression", type)

  #---------

  # Calculate estimates and standard errors for each implicate
  imp.est <- pbapply::pblapply(1:mcnt, FUN = function(m) {

    # Create implicate-specific data.table
    dt <- cbind(implicates[M == m, ], static)

    #---

    if (type == "mean") {

      # When replicate weights are available...
      # Calculate new variables ('.t' suffix) with product of response value and the replicate weights
      # This allows subsequent, by-group calculations to be faster
      if (!is.null(wrep)) {
        for (j in wrep) set(dt, i = NULL, j = paste0(j, ".t"), value = dt[[j]] * dt[[rvar]])
      }

      # Apply the following function to each subset of 'dt' defined by key(dt)
      #x <- dt[climate == "IECC climate zones 1A-2A"]
      #x <- dt[division == "New England"]
      calcMean <- function(x) {

        # Point estimate of "total"
        pe.tot <- sum(x[[rvar]] * x$.WGT)

        # Point estimate of "mean"
        pe.mu <- pe.tot / sum(x$.WGT)

        if (is.null(wrep)) {

          # Standard error of the mean NOT using replicate weights
          se.mu <- weightedSE(x = x[[rvar]], w = x$.WGT)

          # DEPRECATED: Standard error of the total
          # NOTE: Not 100% sure if this is correct
          #se.tot <- se.mu * sum(x$.WGT)

        } else {

          # Standard error using replicate weights
          # Calculation of variance around the point estimate is equivalent to mse = TRUE in svrepdesign()
          reptot <- paste0(wrep, ".t")
          tot <- colSums(x[, ..reptot])  # Sum of 'rvar' value times replicate weights, for each replicate

          # DEPRECATED: Standard error of the total
          ##se.tot <- sqrt(var.scale * sum((tot - mean(tot)) ^ 2) / length(tot))  # SE about the replicate mean
          #se.tot <- sqrt(var.scale * sum((tot - pe.tot) ^ 2) / length(tot))  # SE about the point estimate

          # Mean value for each replicate
          mu <- tot / colSums(x[, ..wrep])

          # Standard error of the mean
          #se.mu <- sqrt(var.scale * sum((mu - mean(mu)) ^ 2) / length(tot)) # SE about the replicate mean
          se.mu <- sqrt(var.scale * sum((mu - pe.mu) ^ 2) / length(tot)) # SE about the point estimate

        }

        data.table(metric = "mean",
                   #metric = c("mean", "total"),
                   #est = c(pe.mu, pe.tot),
                   #se = c(se.mu, se.tot),
                   est = pe.mu,  # Totals removed
                   se = se.mu,  # Totals removed
                   nobs = nrow(x))

      }

      # Apply calcMean(), by group
      out <- dt[, calcMean(.SD), by = by]

    }

    #---

    if (type == "proportion") {

      if (!is.null(wrep)) {

        dt[, nobs := 1L]
        dt <- dt[, lapply(.SD, sum), .SDcols = c("nobs", ".WGT", wrep), by = c(by, rvar)]
        f <- function(x) x / sum(x)
        dt <- dt[, c(".WGT", wrep) := lapply(.SD, f), .SDcols = c(".WGT", wrep), by = by]

        # Replace 'wrep' columns with absolute difference between replicate proportion and point estimate
        for (j in wrep) set(dt, i = NULL, j = j, value = dt[[j]] - dt[[".WGT"]])

        dt[, est := .WGT]
        temp <- rowSums(dt[, ..wrep] ^ 2)
        dt[, se := sqrt(var.scale * temp / length(wrep))]

      } else {

        # In absence of replicate weights...
        # https://stats.stackexchange.com/questions/159204/how-to-calculate-the-standard-error-of-a-proportion-using-weighted-data
        dt[, .WGT := .WGT / sum(.WGT), by = by]
        dt[, w2 := sum(.WGT ^ 2), by = by]
        dt <- dt[, list(est = sum(.WGT), w2 = w2[1], nobs = length(.WGT)), by = c(by, rvar)]
        dt[, se := sqrt(est * (1 - est) * w2)]

      }

      out <- setnames(dt, old = rvar, new = "level")

    }

    #---

    if (type == "regression") {

      # Apply regression across replicate weights
      fitRegression <- function(x) {

        if (!is.null(wrep)) {

          # Generate the design matrix only once
          mm <- model.matrix(object = formula, data = x)

          # Get the point estimate coefficients
          est <- coef(glm.fit(x = mm, y = x[[rvar]], weights = x$.WGT))

          # Compare to 'pe' to confirm model matrix is working
          #coef(glm(formula, data = x, weights = x[["weight"]]))

          # Loop glm.fit() call over the replicate weights
          p <- lapply(wrep, function(w) {
            coef(glm.fit(x = mm, y = x[[rvar]], weights = x[[w]]))
          }) %>%
            bind_rows()
          stopifnot(all(names(est) == names(p)))

          # Standard error across replicate weights
          # If a term is dropped due to now observations, it shows up as NA in 'p' and 'pe'
          se <- sqrt(var.scale * rowSums((t(p) - est) ^ 2) / nrow(p))

        } else {

          # NOTE: appears lm() is generally faster than glm(); may want to replace glm above with lm?
          fit <- lm(formula, data = x, weights = .WGT)
          est <- coef(fit)
          se <- sqrt(diag(vcov(fit, complete = TRUE)))  # This is safe for cases where NA's are returned for coefficients; not true of summary.lm()

        }

        data.table(metric = "coef",
                   term = names(est),
                   est = est,
                   se = se,
                   nobs = nrow(x))
      }

      out <- dt[, fitRegression(.SD), by = by]

    }

    #---

    # Assemble output
    out <- out %>%
      mutate(response = rvar) %>%
      select(all_of(by), response, any_of(c("metric", "level", "term")), est, se, nobs)

    return(out)

  }, cl = cores) %>%
    bind_rows(.id = "M")

  # NOTE: Not clear how to handle cases where a regression term is not present for a particular model
  # Should the coefficient be zero? Or just ignored? Currently just ignored and uncertainty calculated using non-missing implicates

  #---------

  # Identify which columns in 'imp.est' to group by below
  grp <- intersect(names(imp.est), c("response", "metric", "level", "term"))
  #for (v in grp) imp.est[[v]] <- factor(imp.est[[v]], levels = unique(imp.est[[v]]))  # This is done to retain ordering in group_by, below

  # Calculate pooled estimates and standard errors using Reiter (2003) pooling rules for synthetic data
  # The calculations are taken from the mice package: https://github.com/amices/mice/blob/master/R/pool.R
  # Analogous calculations are found in the synthpop package: https://github.com/cran/synthpop/blob/master/R/methods.syn.r
  # See ?summary.fit.synds
  # When M = 1, degrees of freedom is equal to the number of replicate weights; otherwise, calculated using Reiter's formula
  result <- imp.est %>%
    group_by_at(c(by, grp)) %>%
    summarize(
      nobs = mean(nobs),  # Mean number of un-weighted observations per implicate
      m = n(),  # Number of implicates for which there are data
      estimate = mean(.data$est),  # Pooled complete data estimate ('Qbar')

      # Standard error -- sqrt(t) -- calculation using Rubin's (1987) rules (https://cran.r-project.org/web/packages/mitml/vignettes/Analysis.html
      # See "pool results" section here: https://github.com/cran/mitml/blob/master/R/testEstimates.R
      # And for references: https://www.rdocumentation.org/packages/mitml/versions/0.4-3/topics/testEstimates
      ubar = mean(.data$se ^ 2) * N / donor.N,  # Within-implicate variance of estimate ('Ubar'); adjusted for sample size
      b = ifelse(m == 1, 0, var(.data$est)),  # Between-implicate variance of estimate (zero when M = 1)
      t = ubar + (1 + m ^ (-1)) * b,  # Total variance, of estimate; the square root of 't' is the standard error (see below)
      r = (1 + m ^ (-1)) * b / ubar,

      # Rubin's degrees of freedom assuming infinite sample size
      degf = (m - 1) * (1 + r ^ (-1)) ^ 2,

      # Barnard & Rubin (1999) correction of degrees of freedom for finite sample
      # Technically, the 'nobs' entry here should be nobs - k, where k is the number of terms in the analysis (effect is small, though)
      lam = r / (r + 1),
      vobs = (1 - lam) * ((nobs + 1) / (nobs + 3)) * nobs,
      degf = (degf ^ (-1) + vobs ^ (-1)) ^ (-1),

      # Account for single implicate case
      degf = ifelse(m == 1, nobs - 1, degf),

      .groups = "drop"
    ) %>%
    mutate(
      std_error = sqrt(t),  # !!! Standard error (Reiter)
      lower_ci = estimate + ifelse(degf == 0, qnorm(0.025), qt(0.025, df = degf)) * std_error, # 95% confidence interval lower bound
      upper_ci = estimate + ifelse(degf == 0, qnorm(0.975), qt(0.975, df = degf)) * std_error,  # 95% confidence interval upper bound
      statistic = estimate / std_error,  # t-statistic (or z-statistic if proportion)
      pvalue = 2 * ifelse(degf == 0, pnorm(abs(statistic), lower.tail = FALSE), pt(abs(statistic), df = degf, lower.tail = FALSE)), # p-value; Pr(>|t|)
      statistic = round(statistic, 5),
      pvalue = round(pvalue, 5),
      degf = as.integer(round(degf)),
      nobs = as.integer(round(nobs))
    ) %>%
    select(all_of(c(by, grp)), estimate, std_error, lower_ci, upper_ci, statistic, pvalue, degf, nobs) %>%
    select(-any_of("by..placeholder"))  # Drop the by-group placeholder, if present

  #---------

  # Constrain CI's to (0, 1) when reporting proportions
  if (type == "proportion") {
    result <- result %>%
      mutate(lower_ci = pmax(0, lower_ci),
             upper_ci = pmin(1, upper_ci))
  }

  # Constraint lower CI to zero if the response variable is strictly positive when reporting means
  if (type == "mean") {
    pos <- all(!any(implicates[[rvar]] < 0) & !any(static[[rvar]] < 0))  # Is the response variable strictly positive?
    if (pos) result <- mutate(result, lower_ci = pmax(0, lower_ci))
  }

  return(result)

}

#--------

# Standard error of a weighted mean
# Code: https://stats.stackexchange.com/questions/25895/computing-standard-error-in-weighted-mean-estimation
# Computes the variance of a weighted mean following Cochran 1977 definition
# Original paper: https://www.sciencedirect.com/science/article/abs/pii/135223109400210C
weightedSE <- function(x, w, na.rm = FALSE) {
  if (na.rm) {
    i <- !is.na(x) & !is.na(w)
    x <- x[i]
    w <- w[i]
  }
  n <- length(x)
  xWbar = wmean(x, w)
  wbar = mean(w)
  var.se = n/((n-1)*sum(w)^2)*(sum((w*x-wbar*xWbar)^2)-2*xWbar*sum((w-wbar)*(w*x-wbar*xWbar))+xWbar^2*sum((w-wbar)^2))
  return(sqrt(var.se))
}
