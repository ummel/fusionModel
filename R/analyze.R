#' Analyze synthetic data to calculate point estimates and uncertainty
#'
#' @description
#' Calculation of point estimates and associated standard errors for analyses using synthetic microdata. Analysis is currently limited to means, totals, and proportions, (optionally) calculated for population subgroups. Plan to extend to regression model coefficients and custom microsimulation analyses in future. User must supply a list of synthetic implicates (typically generated by \link{fuseM}) and "static" (non-synthetic) variables required by the analysis, including sampling and replicate weights.
#'
#' @param formula A formula expression describing the desired analysis. Example: \code{var1 ~ 1} requests mean and totals if \code{var1} is numeric or proportions if \code{var1} is a factor.
#' @param by Character. Column name(s) in \code{synthetic} or \code{static} (typically factors) that collectively define the population subgroups to analyze.
#' @param synthetic List of data frames containing synthetic implicates, each with the same number of rows as \code{static}. Typically generated by \link{fuseM}.
#' @param static Data frame containing static (non-synthetic) variables with the same number of rows as \code{synthetic}. These data are "static" since they do not vary across implicates. At a minimum, this must include a column named \code{"weight"} giving the primary sampling weights and at least two columns of replicate weights denoted \code{"rep_1"}, \code{"rep_2"}, etc.
#' @param donor.N Number of (un-weighted) observations in the original "donor" data used to generate the implicates.
#' @param syn.N Number of (un-weighted) observations in the "recipient" data. Usually \code{nrow(static)}, the default.
#' @param var.scale Value by which to scale the standard variance calculated across replicate weights. This is specified by the survey design. \code{var.scale = 4} (the default) for both RECS and ACS.
#' @param cores Integer. Number of cores used for parallel operations.
#'
#' @details The requested analysis is performed separately for each implicate. The final point estimate is the mean estimate across implicates. The final standard error is pooled across implicates using Reiter's pooling rules for partially synthetic data (Reiter, 2003). The estimate and standard error of each implicate is calculated using the provided observation weights. Calculations utilize custom code leveraging \code{\link[data.table]{data.table}} operatons for speed and memory efficiency.
#' @details The within-implicate variance is calculated around the point estimate (rather than around the mean of the replicates). This is equivalent to \code{mse = TRUE} in \code{\link[survey]{svrepdesign}}. This seems to be the appropriate method for most surveys.
#'
#' @return A tibble reporting results for the \code{response} variable across population subgroups of \code{by}. The returned quantities are:
#' @return \describe{
#'   \item{estimate}{Point estimate. Mean of estimates across the implicates.}
#'   \item{std_error}{Pooled standard error of the point estimate.}
#'   \item{lower_ci}{Lower bound of 95% confidence interval.}
#'   \item{upper_ci}{Upper bound of 95% confidence interval.}
#'   \item{degf}{Degrees of freedom of t-distribution if calculating custom confidence intervals.}
#'   \item{nobs}{Mean number of observations per strata across the implicates.}
#'   }
#'
#' @references
#' Reiter, J.P. (2003). Inference for Partially Synthetic,
#' Public Use Microdata Sets. \emph{Survey Methodology}, \bold{29}, 181-189.
#'
#' @examples
#' # Build a fusion model using RECS microdata
#' fusion.vars <- c("electricity", "natural_gas", "aircon")
#' predictor.vars <- names(recs)[2:12]
#' fit <- train(data = recs, y = fusion.vars, x = predictor.vars)
#'
#' # Generate 5 implicates of synthetic 'fusion.vars',
#' #  using original RECS data as the recipient
#' recipient <- recs[predictor.vars]
#' sim <- fuseM(data = recipient, train.object = fit, M = 5)
#'
#' # Analyze electricity consumption, by climate zone
#' result <- analyze(formula = electricity ~ 1,
#'                   by = "climate",
#'                   synthetic = sim,
#'                   static = recs,
#'                   donor.N = nrow(recs))
#'
#' # Analyze air conditioning technology, by race
#' result <- analyze(formula = aircon ~ 1,
#'                   by = "race",
#'                   synthetic = sim,
#'                   static = recs,
#'                   donor.N = nrow(recs))
#'
#' # Analyze natural gas consumption across whole sample
#' result <- analyze(formula = natural_gas ~ 1,
#'                   synthetic = sim,
#'                   static = recs,
#'                   donor.N = nrow(recs))
#'
#' # If a single implicate is provided, calculation
#' #  proceeds but with a useful warning
#' result <- analyze(formula = natural_gas ~ 1,
#'                   synthetic = sim[1],
#'                   static = recs,
#'                   donor.N = nrow(recs))
#'
#'#----------
#'
#'# Validation of internal calculations
#'# If single implicate is provided, the results should
#'#  match standard replicate weight calculations
#'
#'# Analyze with all implicates equal to first implicate
#'result <- analyze(
#'  formula = electricity ~ 1,
#'  by = "climate",
#'  synthetic = sim[1],
#'  static = recs,
#'  donor.N = nrow(recs)
#') %>%
#'filter(metric == "mean")
#'
#'# Result of standard replicate weight calculation,
#'#  as implemented within "survey" package
#'recs.design <- survey::svrepdesign(
#'  data = cbind(recipient, sim[[1]]),
#'  weights = recs$weight,
#'  repweights = select(recs, starts_with("rep_")),
#'  type = "Fay",
#'  rho = 0.5,
#'  mse = FALSE
#')
#'check <- survey::svyby(
#'  formula = electricity ~ 1,
#'  by = ~ climate,
#'  FUN = survey::svymean,
#'  design = recs.design
#')
#'
#'# Compare 'result' with 'check'
#'all.equal(result$estimate, check$electricity)
#'all.equal(result$std_error, check$se)
#' @export
#-----

analyze <- function(formula,
                    by,
                    synthetic,
                    static,
                    donor.N,
                    syn.N = nrow(static),
                    var.scale = 4,
                    cores = 1) {

  if (missing(by)) by <- NULL

  stopifnot({
    class(formula) == "formula"
    is.null(by) | is.character(by)
    is.list(synthetic)
    is.data.frame(static)
    donor.N > 0 & donor.N %% 1 == 0
    syn.N > 0 & syn.N %% 1 == 0
    is.numeric(var.scale)
    cores > 0 & cores %% 1 == 0
    all(sapply(synthetic, nrow) == nrow(static))
    all(c("weight", "rep_1", "rep_2") %in% names(static))
  })

  # Variance adjustment factor based on synthetic sample size relative to original data
  # This is used to inflate the within-imputation variance when syn.N and donor.N are different
  #vaf <- syn.N / donor.N

  # Determine variables used within 'formula'; will throw error if all required variables cannot be located
  dtemp <- cbind(synthetic[[1]][1, ], static[1, ])
  fvars <- names(get_all_vars(formula, data = dtemp))

  # Retain only necessary columns in the input data
  # Conversion of numeric to double is to prevent any data type issues with data.table
  synthetic <- synthetic %>%
    map(~ .x %>%
          select(any_of(c(fvars, by))) %>%
          mutate_if(is.numeric, as.double) %>%
          as.data.table())

  static <- static %>%
    select(any_of(setdiff(c(fvars, by), names(synthetic[[1]]))), weight, starts_with("rep_")) %>%
    mutate_if(is.numeric, as.double) %>%
    as.data.table()

  # Names of the replicate weights in 'static'
  # Note that the primary weight variable is assumed to be called "weight"
  wrep <- names(select(static, starts_with("rep_")))

  # Set 'by' to NULL is not specified
  if (is.null(by)) {
    by <- "by..placeholder"
    static$by..placeholder <- 1L
  }

  # Calculation type
  rvar <- fvars[1]  # The "response" variable
  type <- ifelse("factor" %in% class(dtemp[[rvar]]) | is.logical(dtemp[[rvar]]), "proportion", "mean")
  if (type == "factor" & length(fvars) > 1) stop("Factor variable not allowed as dependent variable in regression models")
  type <- ifelse(length(fvars) > 1, "model", type)

  #---------

  # Apply the following function to each subset of 'dt' defined by key(dt)
  #x <- dt[climate == "IECC climate zones 1A-2A"]
  calcMean <- function(x) {

    # Point estimate of "total"
    pe.tot <- sum(x[[rvar]] * x$weight)

    # Point estimate of "mean"
    pe.mu <- pe.tot / sum(x$weight)

    # Standard error across implicates
    # This calculation uses mean(tot) instead of 'pe.tot' for calculation of variance
    # This is equivalent to setting mse = FALSE in svrepdesign()
    reptot <- paste0(wrep, ".t")
    tot <- colSums(x[, ..reptot])  # Sum total for each replicate
    #se.tot <- sqrt(var.scale * sum((tot - mean(tot)) ^ 2) / length(tot))  # SE about the replicate mean
    se.tot <- sqrt(var.scale * sum((tot - pe.tot) ^ 2) / length(tot))  # SE about the point estimate

    mu <- tot / colSums(x[, ..wrep])  # Mean values across replicates
    #se.mu <- sqrt(var.scale * sum((mu - mean(mu)) ^ 2) / length(tot)) # SE about the replicate mean
    se.mu <- sqrt(var.scale * sum((mu - pe.mu) ^ 2) / length(tot)) # SE about the point estimate

    out <- data.table(metric = c("mean", "total"),
                      est = c(pe.mu, pe.tot),
                      se = c(se.mu, se.tot),
                      nobs = nrow(x))

    return(out)

  }

  #---------

  # Calculate estimates and standard errors for each implicate
  imp.est <- pbapply::pblapply(synthetic, FUN = function(syn.data) {

    # Create implicate-specific data.table
    dt <- cbind(syn.data, static)

    if (type == "mean") {

      # Calculate new variables ('.t' suffix) with product of response value and the replicate weights
      for (j in wrep) set(dt, i = NULL, j = paste0(j, ".t"), value = dt[[j]] * dt[[rvar]])

      # Apply calcMean(), by group
      out <- dt[, calcMean(.SD), by = by]

    }

    if (type == "proportion") {

      rlev <- levels(dt[[rvar]])
      set(dt, i = NULL, j = "Nobs", value = 1L) # Nobs: number of observations in each strata
      dt <- dt[, lapply(.SD, sum), .SDcols = c("Nobs", "weight", wrep), by = c(by, rvar)]
      #dt <- dt[, lapply(.SD, sum), .SDcols = c("weight", wrep), by = c(by, rvar)]

      f <- function(x) x / sum(x)
      dt <- dt[, c("weight", wrep) := lapply(.SD, f), .SDcols = c("weight", wrep), by = by]

      # Replace 'wrep' columns with absolute difference between replicate proportion and point estimate
      for (j in wrep) set(dt, i = NULL, j = j, value = dt[[j]] - dt[["weight"]])

      out <- dt %>%
        mutate(se = sqrt(var.scale * rowSums(dt[, ..wrep] ^ 2) / length(wrep)),
               #se = sqrt(var.scale * rowSums((dt[, ..wrep] - rowMeans(dt[, ..wrep])) ^ 2) / length(wrep)),
               metric = "proportion") %>%
        rename(nobs = Nobs,
               est = weight,
               level = !!rvar)

    }

    # Assemble output
    out <- out %>%
      mutate(response = rvar) %>%
      select(all_of(by), response, any_of(c("metric", "level")), est, se, nobs)

    return(out)

  }, cl = cores) %>%
    bind_rows(.id = "M")

  #---------

  # Identify which columns in 'imp.est' to group by below
  grp <- intersect(names(imp.est), c("response", "metric", "level", "term"))
  for (v in grp) imp.est[[v]] <- factor(imp.est[[v]], levels = unique(imp.est[[v]]))

  # Calculate pooled estimates and standard errors using Reiter (2003) pooling rules for synthetic data
  # The calculations are taken from the mice package: https://github.com/amices/mice/blob/master/R/pool.R
  # Analogous calculations are found in the synthpop package: https://github.com/cran/synthpop/blob/master/R/methods.syn.r
  # See ?summary.fit.synds
  # When M = 1, degrees of freedom is equal to the number of replicate weights; otherwise, calculated using Reiter's formula
  result <- imp.est %>%
    group_by_at(c(by, grp)) %>%
    summarize(
      nobs = mean(nobs),
      m = n(),  # Number of implicates for which there are data
      estimate = mean(.data$est),  # Pooled complete data estimate
      ubar = mean(.data$se ^ 2) * syn.N / donor.N,  # Within-imputation variance of estimate; ADJUSTED for sample size
      b = ifelse(m == 1, 0, var(.data$est)),  # Between-imputation variance of estimate (zero when M = 1)
      t = ubar + (1 / m) * b,  # Total variance, of estimate; the square root of 't' is the standard error (see below)
      degf = ifelse(m == 1, length(wrep), (m - 1) * (1 + (ubar / (b / m))) ^ 2),  # Degrees of freedom of t-statistic
      .groups = "drop"
    ) %>%
    mutate(
      std_error = sqrt(t),  # Standard error
      lower_ci = estimate + qt(0.025, df = degf) * std_error, # 95% confidence interval lower bound
      upper_ci = estimate + qt(0.975, df = degf) * std_error  # 95% confidence interval upper bound
    ) %>%
    select(all_of(c(by, grp)), estimate, std_error, lower_ci, upper_ci, degf, nobs) %>%
    select(-any_of("by..placeholder"))  # Drop the by-group placeholder, if present

  #---------

  # Constrain CI's to (0, 1) when proportions are being reported
  if (type == "proportion") {
    result <- result %>%
      mutate(lower_ci = pmax(0, lower_ci),
             upper_ci = pmin(1, upper_ci))
  }

  # Constraint lower CI to zero if the response variable is strictly positive
  if (type == "mean") {
    pos <- all(!map_lgl(synthetic, ~ any(.x[[rvar]] < 0))) & !any(static[[rvar]] < 0)  # Is the response variable strictly positive?
    if (pos) result <- mutate(result, lower_ci = pmax(0, lower_ci))
  }

  # If a small number of implicates are provided, issue appropriate warnings
  if (length(synthetic) == 1) {
    warning("M = 1. Results reflect standard replicate weight calculations (no pooling across implicates).")
  } else {
    if (length(synthetic) < 5) warning("Only ", length(synthetic), " implicates provided. Reliability of results increases with the number of implicates.")
  }

  return(result)

}

#--------

# # DEPRECATED -- but may want to pull bits of this code in future
# # # Function to calculate estimate and standard error, given a formula, data frame, and specified type of calculation
# # # 'type' can be "mean", "proportion", or "model"
# estCalc <- function(formula, data, type) {
#
#   if (type == "mean") {
#     x <- data[[1]]
#     w <- data$weight
#     out <- tibble(response = names(data)[1],
#                   metric = c("mean", "total"),
#                   estimate = wmu(x, w) * c(1, sum(w)),  # Mean and Total
#                   std.error = (wsd(x, w) / sqrt(nrow(data))) * c(1, sum(w)),
#                   df = nrow(data) - 1L)
#   }
#
#   if (type == "proportion") {
#     p <- xtabs(as.formula(paste0("weight~", names(data)[1])), data = data)
#     p <- as.numeric(p / sum(p))
#     out <- tibble(response = names(data)[1],
#                   level = if (is.logical(data[[1]])) c(FALSE, TRUE) else levels(data[[1]]),
#                   estimate = p,
#                   std.error = sqrt(estimate * (1 - estimate) / nrow(data)),
#                   df = nrow(data) - 1L)
#   }
#
#   # tidy generics: https://cran.r-project.org/web/packages/generics/readme/README.html
#   if (type == "model") {
#     out <- suppressWarnings(tidyGLM(glm(formula = formula, data = data, weights = weight))) %>%
#       mutate(response = names(data)[1]) %>%
#       select(response, term, estimate, std.error, df)
#   }
#
#   return(out)
#
# }
#
# #--------
#
# # Function to return "tidied" glm() results
# # Based on function in broom package: https://github.com/tidymodels/broom/blob/main/R/stats-glm-tidiers.R
#
# tidyGLM <- function(x) {
#   xsummary <- summary(x)
#   ret <- as_tibble(xsummary$coefficients, rownames = "term")
#   colnames(ret) <- c("term", "estimate", "std.error", "statistic", "p.value")
#   ret$df <- xsummary$df.residual  # Add degrees of freedom
#   coefs <- tibble::enframe(stats::coef(x), name = "term", value = "estimate")
#   ret <- left_join(coefs, ret, by = c("term", "estimate"))
#   return(ret)
# }
