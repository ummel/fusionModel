#' Analyze synthetic data to calculate point estimates and uncertainty
#'
#' @description
#' Calculation of point estimates and associated standard errors for analyses using synthetic microdata. Analysis is currently limited to means, totals, and proportions, (optionally) calculated for population subgroups. Will extend to regression model coefficients in future. User must supply a list of synthetic implicates (typically generated by \link{fuseM}), as well as any "static" (non-synthetic) variables required by the analysis and a survey "design" object containing observation weights and information about how to employ replicate weights (see \code{\link[survey]{svrepdesign}}).
#'
#' @param formula A formula expression describing the desired analysis. Example: \code{var1 ~ 1} requests mean and totals if \code{var1} is numeric or proportions if \code{var1} is a factor.
#' @param by Character. Column name(s) in \code{synthetic} or \code{static} (typically factors) that collectively define the population subgroups to analyze.
#' @param synthetic List of data frames containing synthetic implicates, each with the same number of rows as \code{static}. Typically generated by \link{fuseM}.
#' @param static Data frame containing static (non-synthetic) variables with the same number of rows as \code{synthetic}. These data are "static" since they do not vary across implicates.
#' @param design Object of class \code{"svyrep.design"} generated by \code{\link[survey]{svrepdesign}}. See Examples.
#' @param donor.N Number of (un-weighted) observations in the original "donor" data used to generate the implicates.
#' @param cores Integer. Number of cores used for parallel operations.
#'
#' @details The requested analysis is performed separately for each implicate. The final point estimate is the mean estimate across implicates. The final standard error is pooled across implicates using Reiter's pooling rules for partially synthetic data (Reiter, 2003). The estimate and standard error of each implicate is calculated by \code{\link[survey]{svyby}} using the provided observation weights and sample design.
#'
#' @return A tibble reporting results for the \code{response} variable across population subgroups of \code{by}. The returned quantities are:
#' @return \describe{
#'   \item{estimate}{Point estimate. Mean of estimates across the implicates.}
#'   \item{std_error}{Pooled standard error of the point estimate.}
#'   \item{lower_ci}{Lower bound of 95% confidence interval.}
#'   \item{upper_ci}{Upper bound of 95% confidence interval.}
#'   \item{degf}{Degrees of freedom of t-distribution if calculating custom confidence intervals.}
#'   }
#'
#' @references
#' Reiter, J.P. (2003). Inference for Partially Synthetic,
#' Public Use Microdata Sets. \emph{Survey Methodology}, \bold{29}, 181-189.
#'
#' @examples
#' # Build a fusion model using RECS microdata
#' fusion.vars <- c("electricity", "natural_gas", "aircon")
#' predictor.vars <- names(recs)[2:12]
#' fit <- train(data = recs, y = fusion.vars, x = predictor.vars)
#'
#' # Generate 5 implicates of synthetic 'fusion.vars',
#' #  using original RECS data as the recipient
#' recipient <- recs[predictor.vars]
#' sim <- fuseM(data = recipient, train.object = fit, M = 5)
#'
#' # Specify the survey design of the recipient microdata
#' # Settings are survey-specific; see page 9 here:
#' # https://www.eia.gov/consumption/residential/data/2015/pdf/microdata_v3.pdf
#' recs.design <- survey::svrepdesign(weights = recs$weight,
#'                                    type = "Fay",
#'                                    rho = 0.5,
#'                                    repweights = select(recs, starts_with("rep_")),
#'                                    mse = TRUE)
#'
#' # Analyze electricity consumption, by climate zone
#' result <- analyze(formula = electricity ~ 1,
#'                   by = "climate",
#'                   synthetic = sim,
#'                   static = recipient,
#'                   design = recs.design,
#'                   donor.N = nrow(recs))
#' head(result)
#' @export
#-----

analyze <- function(formula,
                    by,
                    synthetic,
                    static,
                    design,
                    donor.N,
                    cores = 1) {

  if (missing(by)) by <- NULL

  stopifnot({
    class(formula) == "formula"
    is.null(by) | is.character(by)
    is.list(synthetic)
    length(synthetic) > 1
    is.data.frame(static)
    class(design) == "svyrep.design"
    donor.N > 0 & donor.N %% 1 == 0
    cores > 0 & cores %% 1 == 0
    all(sapply(synthetic, nrow) == nrow(static))
  })

  # Set 'by' to NULL is not specified
  if (is.null(by)) {
    by <- "by..placeholder"
    static$by..placeholder <- 1L
  }

  # Number of observations in the provided data
  syn.N <- nrow(static)

  # Variance adjustment factor based on synthetic sample size relative to original data
  # This is used to inflate the within-imputation variance when syn.N and donor.N are different
  vaf <- syn.N / donor.N

  # Determine variables used within 'formula'; will throw error if all required variables cannot be located
  dtemp <- cbind(synthetic[[1]][1, ], static[1, ])
  fvars <- names(get_all_vars(formula, data = dtemp))

  # Retain only necessary columns in the input data
  synthetic <- map(synthetic, ~ select(.x, any_of(c(fvars, by))))
  static <- select(static, any_of(setdiff(c(fvars, by), names(synthetic[[1]]))))
  gc()

  # Set variables slot to NULL; it is replaced within the implicates loop
  design$variables <- NULL

  # Calculation type
  rvar <- fvars[1]  # The "response" variable
  type <- ifelse("factor" %in% class(dtemp[[rvar]]) | is.logical(dtemp[[rvar]]), "proportion", "mean")
  if (type == "factor" & length(fvars) > 1) stop("Factor variable not allowed as dependent variable in regression models")
  type <- ifelse(length(fvars) > 1, "model", type)

  #---------

  # Calculate estimates and standard errors for each implicate
  #imp.estimates <- pbapply::pblapply(1:length(synthetic), FUN = function(i) {
  imp.estimates <- pbapply::pblapply(synthetic, FUN = function(syn.data) {

    # Assign implicate data to the 'design' object
    design$variables <- cbind(syn.data, static)

    # Estimation via survey::svymean()
    out <- survey::svyby(
      formula = formula,
      by = as.formula(paste("~", paste(by, collapse = "+"))),
      FUN = survey::svymean,
      design = design
    ) %>%
      as_tibble()

    # Add estimates of total (sum) if type = 'mean'
    if (type == "mean") {

      total <- survey::svyby(
        formula = formula,
        by = as.formula(paste("~", paste(by, collapse = "+"))),
        FUN = survey::svytotal,
        design = design
      ) %>%
        as_tibble() %>%
        mutate(metric = "total")

      names(total)[length(by) + 1]  <- "estimate"
      names(out)[length(by) + 1]  <- "estimate"

      out <- out %>%
        mutate(metric = "mean") %>%
        bind_rows(total)

    }

    # Process output in 'out' for case where proportions are reported
    # This creates a "level" column
    if (type == "proportion") {

      j <- length(by)
      k <- ncol(out) - j

      est <- out %>%
        select(c(1:(j + k / 2))) %>%
        pivot_longer(cols = -all_of(by), values_to = "estimate")

      se <- out %>%
        select(-seq(j + 1, length.out = k / 2)) %>%
        pivot_longer(cols = -all_of(by), values_to = "se") %>%
        select(se)

      out <- cbind(est, se) %>%
        mutate(level = substring(name, first = nchar(rvar) + 1),
               response = rvar)

    }

    # Assemble output
    out <- out %>%
      mutate(response = rvar) %>%
      select(all_of(by), response, any_of(c("metric", "level")), estimate, se)

    return(out)

  }, cl = cores) %>%
    bind_rows(.id = "M")

  #---------

  # Identify which columns in 'imp.estimates' to group by below
  grp <- intersect(names(imp.estimates), c("response", "metric", "level", "term"))
  for (v in grp) imp.estimates[[v]] <- factor(imp.estimates[[v]], levels = unique(imp.estimates[[v]]))

  # Calculate pooled estimates and standard errors using Reiter (2003) pooling rules for synthetic data
  # The calculations are taken from the mice package: https://github.com/amices/mice/blob/master/R/pool.R
  # Analogous calculations are found in the synthpop package: https://github.com/cran/synthpop/blob/master/R/methods.syn.r
  # See ?summary.fit.synds
  result <- imp.estimates %>%
    group_by_at(c(by, grp)) %>%
    summarize(
      m = n(),  # Number of implicates for which there are data
      qbar = mean(.data$estimate),  # Pooled complete data estimate
      ubar = mean(.data$se ^ 2) * vaf,  # Within-imputation variance of estimate; ADJUSTED for sample size
      b = var(.data$estimate),  # Between-imputation variance of estimate
      t = ubar + (1 / m) * b,  # Total variance, of estimate; the square root of 't' is the standard error (see below)
      df = (m - 1) * (1 + (ubar / (b / m))) ^ 2,  # Degrees of freedom of $t$-statistic
      .groups = "drop"
    ) %>%
    rename(
      estimate = qbar,
      degf = df
    ) %>%
    mutate(
      std_error = sqrt(t),  # Standard error
      lower_ci = estimate + qt(0.025, df = degf) * std_error, # 95% confidence interval lower bound
      upper_ci = estimate + qt(0.975, df = degf) * std_error  # 95% confidence interval upper bound
    ) %>%
    select(all_of(c(by, grp)), estimate, std_error, lower_ci, upper_ci, degf) %>%
    select(-any_of("by..placeholder"))  # Drop the by-group placeholder, if present

  #---------

  # Constrain CI's to (0, 1) when proportions are being reported
  if ("level" %in% names(result)) {
    result <- result %>%
      mutate(lower_ci = pmax(0, lower_ci),
             upper_ci = pmin(1, upper_ci))
  }

  # Constraint lower CI to zero if the response variable is strictly positive
  if ("metric" %in% names(result)) {
    # Is the response variable strictly positive?
    pos <- all(!map_lgl(synthetic, ~ any(.x[[rvar]] < 0))) & !any(static[[rvar]] < 0)
    if (pos) {
      result <- result %>%
        mutate(lower_ci = pmax(0, lower_ci))
    }
  }

  return(result)

}

#--------

# DEPRECATED -- but may want to pull bits of this code in future
# # Function to calculate estimate and standard error, given a formula, data frame, and specified type of calculation
# # 'type' can be "mean", "proportion", or "model"
# estCalc <- function(formula, data, type) {
#
#   if (type == "mean") {
#     x <- data[[1]]
#     w <- data$weight
#     out <- tibble(response = names(data)[1],
#                   metric = c("mean", "total"),
#                   estimate = wmu(x, w) * c(1, sum(w)),  # Mean and Total
#                   std.error = (wsd(x, w) / sqrt(nrow(data))) * c(1, sum(w)),
#                   df = nrow(data) - 1L)
#   }
#
#   if (type == "proportion") {
#     p <- xtabs(as.formula(paste0("weight~", names(data)[1])), data = data)
#     p <- as.numeric(p / sum(p))
#     out <- tibble(response = names(data)[1],
#                   level = if (is.logical(data[[1]])) c(FALSE, TRUE) else levels(data[[1]]),
#                   estimate = p,
#                   std.error = sqrt(estimate * (1 - estimate) / nrow(data)),
#                   df = nrow(data) - 1L)
#   }
#
#   # tidy generics: https://cran.r-project.org/web/packages/generics/readme/README.html
#   if (type == "model") {
#     out <- suppressWarnings(tidyGLM(glm(formula = formula, data = data, weights = weight))) %>%
#       mutate(response = names(data)[1]) %>%
#       select(response, term, estimate, std.error, df)
#   }
#
#   return(out)
#
# }
#
# #--------
#
# # Function to return "tidied" glm() results
# # Based on function in broom package: https://github.com/tidymodels/broom/blob/main/R/stats-glm-tidiers.R
#
# tidyGLM <- function(x) {
#   xsummary <- summary(x)
#   ret <- as_tibble(xsummary$coefficients, rownames = "term")
#   colnames(ret) <- c("term", "estimate", "std.error", "statistic", "p.value")
#   ret$df <- xsummary$df.residual  # Add degrees of freedom
#   coefs <- tibble::enframe(stats::coef(x), name = "term", value = "estimate")
#   ret <- left_join(coefs, ret, by = c("term", "estimate"))
#   return(ret)
# }
